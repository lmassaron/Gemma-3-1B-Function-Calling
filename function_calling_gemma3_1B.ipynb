{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lmassaron/Gemma-3-1B-Function-Calling/blob/main/function_calling_gemma3_1B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppSo_A50d7Wo"
   },
   "source": [
    "We start this tutorial on fine-tuning a Gemma 3 mode for function calling by installing or updating the necessary Python packages required for the process. The necessary packages for running the code are:\n",
    "\n",
    "- `transformers`: Provides access to pre-trained a large range of language models (like Gemma), tokenizers, and training utilities from Hugging Face.\n",
    "- `accelerate`: Simplifies running PyTorch code on various hardware setups (CPU, single/multi-GPU, TPU) and handles mixed-precision training.\n",
    "- `datasets`: Used for efficiently loading, processing, and manipulating datasets, especially those hosted on the Hugging Face Hub.\n",
    "- `peft`: (Parameter-Efficient Fine-Tuning) Enables techniques like LoRA (Low-Rank Adaptation) to fine-tune large models efficiently by training only a small number of extra parameters.\n",
    "- `trl`: (Transformer Reinforcement Learning library) Provides tools for fine-tuning language models, including the `SFTTrainer` used here for Supervised Fine-Tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C6pflsoAd-CQ"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U transformers\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AV3ibJueKx0"
   },
   "source": [
    "Then we proceed importing the required modules and classes from the installed libraries and Python's standard library. In particular take notice about:\n",
    "\n",
    "- `os`: Used for interacting with the operating system.\n",
    "- `enum.Enum`: Used to create enumeration types, here specifically for defining special tokens in a structured way.\n",
    "- `torch`: The core PyTorch library for tensor computations and neural network modules.\n",
    "- `transformers`: Imports `AutoModelForCausalLM` (to load the language model), `AutoTokenizer` (to load the tokenizer), and `set_seed` (for reproducibility).\n",
    "- `datasets`: Imports `load_dataset` for fetching data from the Hugging Face Hub.\n",
    "- `trl`: Imports `SFTConfig` (configuration for supervised fine-tuning) and `SFTTrainer` (the class that handles the training process).\n",
    "- `peft`: Imports `LoraConfig` (configuration for LoRA) and `TaskType` (to specify the type of task for PEFT, e.g., Causal LM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BepmFj8Pd-CQ"
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, TaskType\n",
    "from peft import PeftModel, PeftConfig\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lW35X1iC0vb"
   },
   "source": [
    "We then start trying to make all the process deterministic, using the helper utility set_seed that sets the random seed for Python's random, numpy, torch (across various devices), and tensorflow (if available) to ensure reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "x408hJAk8IW8"
   },
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRs3NrQ6edeI"
   },
   "source": [
    "This cell defines an `Enum` class `ChatmlSpecialTokens` to manage custom special tokens related to function/tool calling within the ChatML format. A class method is provided to easily retrieve all defined special token values as a list, useful for adding them to the tokenizer.\n",
    "\n",
    "Using an Enum provides a robust and readable way to handle these additional tokens consistently throughout the script, avoiding potential typos with raw strings. We actually need new tokens in order to better handle how Gemma will refer to calling external functions and the used inputs and parameters and the tool response.\n",
    "These tokens are crucial for training the model to understand and generate text involving tool interactions, as expected by the chosen dataset (`hermes-function-calling-v1`).\n",
    "\n",
    "Here are the additional tokens that we will be using:\n",
    "\n",
    "  - `<tools>`, `</tools>`: Delimit a section describing available tools.\n",
    "  - `<think>`, `</think>`: Delimit the model's internal thought process before acting.\n",
    "  - `<tool_call>`, `</tool_call>`: Delimit the model's request to call a specific tool.\n",
    "  - `<tool_response>`, `</tool_response>`: Delimit the response received after executing a tool.\n",
    "  - `<pad>`: Padding token used to make sequences in a batch the same length.\n",
    "  - `<eos>`: End-of-sequence token, often used to signal the end of a generated turn or document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ORgc0JQjd-CR"
   },
   "outputs": [],
   "source": [
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    \"\"\"Enum class defining special tokens used in the ChatML format\"\"\"\n",
    "\n",
    "    tools = \"<tools>\"\n",
    "    eotools = \"</tools>\"\n",
    "    think = \"<think>\"\n",
    "    eothink = \"</think>\"\n",
    "    tool_call = \"<tool_call>\"\n",
    "    eotool_call = \"</tool_call>\"\n",
    "    tool_response = \"<tool_response>\"\n",
    "    eotool_response = \"</tool_response>\"\n",
    "    pad_token = \"<pad>\"\n",
    "    eos_token = \"<eos>\"\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do7Q-E3-e0kV"
   },
   "source": [
    "## Setting the Stage: Configuring Our Fine-Tuning Experiment\n",
    "\n",
    "Before we proceed to fine-tuning our model, we need to lay some groundwork. This next step is all about setting up the \"control panel\" for our fine-tuning script. We'll be using a Python class, which we'll call `Config`, to neatly organize all the different settings and hyperparameters. Grouping all our settings in one place like this offers several advantages:\n",
    "\n",
    "1.  **Organization:** It keeps our script clean and tidy. All configuration options are in one predictable spot.\n",
    "2.  **Readability:** Anyone (including our future selves!) can quickly understand the setup for a particular experiment.\n",
    "3.  **Easy Tweaking:** When we want to experiment with different hyperparameters (like learning rates or model sizes), we only need to change them in this central `Config` class.\n",
    "\n",
    "Let's explore the core components of the class regarding model, data and output. First, let's define the fundamental pieces of our experiment:\n",
    "\n",
    "*   `model_name: \"google/gemma-3-1b-it\"`\n",
    "    * This tells our script which pre-trained model we want to start with. We're choosing \"google/gemma-3-1b-it\" Gemma-3-1B-IT is an instruction-tuned version of Google's Gemma model. \"IT\" stands for Instruction Tuned, meaning it's already been trained to follow instructions well, making it a great candidate for further specialization, like teaching it to use tools or functions. The \"1B\" indicates it has around 270 million parameters.\n",
    "\n",
    "*   `dataset_name: \"lmassaron/hermes-function-calling-v1\"`\n",
    "    * This specifies the dataset we'll use to teach our model its new skills. \"lmassaron/hermes-function-calling-v1\" is a dataset specifically designed for function/tool calling. It contains examples of conversations where a model needs to understand when and how to use external tools.\n",
    "\n",
    "*   `output_dir: \"gemma-3-1b-it-function_calling\"`\n",
    "    * This is simply the name of the folder where our script will save all the important outputs, including things like the trained model adapters (we'll talk about LoRA soon), checkpoints during training, and any evaluation results.\n",
    "\n",
    "We're not going to retrain the entire Gemma model from scratch (that would take a lot of resources), instead, we'll use a technique called **LoRA (Low-Rank Adaptation)**. LoRA is a form of Parameter-Efficient Fine-Tuning (PEFT) that allows us to adapt the model effectively by training only a small number of new parameters.\n",
    "\n",
    "Here are the LoRA-specific settings:\n",
    "\n",
    "*   `lora_arguments`: This is a dictionary holding all our LoRA configurations.\n",
    "    *   `r=16`: This is the **rank** of the LoRA matrices.\n",
    "        * LoRA works by superimposing small, trainable matrices into the existing layers of the model. The rank `r` determines the size (and thus, the expressiveness) of these matrices.\n",
    "        * A higher rank means more trainable parameters and potentially better adaptation, but also more memory and computation. `16` is a common value that often provides a good balance between performance and efficiency. There is no definitive choice on the rank to be used for any situation, you actually have to experiment a bit before finding the right rank to use.\n",
    "    *   `lora_alpha=64`: This is a **scaling factor** for the LoRA adaptation.\n",
    "        * The learned LoRA weights are scaled by `lora_alpha / r` before being added to the original model weights.\n",
    "        * It's often set to be 2x or 4x the rank (`r`). Here, `64` (which is `4 * 16`) provides a relatively strong scaling, influencing how much the LoRA adaptation impacts the model's behavior (in our case we amplify the impact).\n",
    "    *   `lora_dropout=0.05`: This is a **dropout rate** applied specifically to the LoRA layers.\n",
    "        *  Dropout randomly \"turns off\" a fraction of neurons during training, which helps prevent the model from overfitting to the training data.\n",
    "        * `0.05` (or 5%) is a relatively low dropout rate, suggesting we're aiming for a gentle regularization.\n",
    "    *   `target_modules`: This list tells the script *which parts* of the base model should have LoRA adapters injected.\n",
    "            *   `q_proj`, `k_proj`, `v_proj`, `o_proj`: These are the query, key, value, and output projection layers within the model's attention mechanism. Targeting these is standard practice for LoRA and very effective.\n",
    "            *   `gate_proj`, `up_proj`, `down_proj`: These are components of the feed-forward network (FFN) layers in the transformer blocks. Adapting these also contributes significantly to learning.\n",
    "            *   `embed_tokens`, `lm_head`: We're also targeting the input embedding layer (`embed_tokens`) and the final language modeling head (`lm_head`). This can be particularly beneficial if we're introducing new special tokens (as we are doing in this project) or if we want to heavily adapt how the model generates its final output.\n",
    "\n",
    "Now we get to the nitty-gritty of how the actual training loop will behave, that is controlling the training process. These settings are passed to the `SFTTrainer` (Supervised Fine-tuning Trainer) from the TRL library, which itself builds upon the `TrainingArguments` from the Hugging Face `transformers` library.\n",
    "\n",
    "*   `training_arguments`: Another dictionary, this one packed with options to control the training run.\n",
    "    *   `num_train_epochs=1`:\n",
    "        * The model will go through the entire training dataset exactly once. For fine-tuning, especially with efficient methods like LoRA and potentially large datasets, one epoch is often sufficient to achieve good results without overfitting (where the model learns the training data too well but doesn't generalize).\n",
    "    *   `per_device_train_batch_size=1`:\n",
    "        * During training, each GPU will process 1 training example at a time before performing a backward pass (to calculate gradients). This is  chosen due to GPU memory limitations (we can run on Colab using L4 GPUs), especially when using a large `max_seq_length` (which we'll see next). Larger models and longer sequences consume more memory.\n",
    "    *   `gradient_accumulation_steps=4`:\n",
    "        * Instead of updating the model's weights after every single batch (of size 1, in our case), we'll accumulate the gradients from 4 batches. Only then will we perform a weight update.\n",
    "        * This effectively simulates a larger batch size (`1 sample/batch * 4 accumulation_steps = effective batch size of 4`) without the same memory cost. Larger effective batch sizes can lead to more stable training.\n",
    "    *   `max_length=4096`:\n",
    "        * This is the maximum number of tokens that any single input sequence fed to the model can have. Any training examples longer than this will be filtered out. This value significantly impacts GPU memory usage – longer sequences need more memory. `4096` is a reasonable context window for many recent models like Gemma, allowing it to \"see\" a good amount of text at once.\n",
    "    *   `packing=False`:\n",
    "        * This option when true enables a smart trick called \"packing.\" If we have many short sequences in our dataset, instead of padding them all out to `max_length` (which is wasteful), packing combines multiple short sequences into a single input sequence, up to `max_length`. These are typically separated by an End-Of-Sentence (EOS) token. This greatly improves training efficiency by reducing wasted computation on padding tokens. Unfortunately it doesn't works if you are not using flash_attention_2, which is the only attention mechanism that fully supports these features.\n",
    "    *   `optim=\"adamw_torch_fused\"`:\n",
    "        * This specifies the optimizer we'll use. AdamW is an improved version of the popular Adam optimizer. The `_fused` suffix often indicates an implementation that's optimized for faster performance on GPUs.\n",
    "    *   `learning_rate=1e-4` (or `0.0001`):\n",
    "        * This is the initial rate at which the optimizer will adjust the model's weights. `1e-4` is a common and often effective starting learning rate for LoRA fine-tuning.\n",
    "    *   `weight_decay=0.1`:\n",
    "        * This applies L2 regularization (also known as weight decay) to the model's weights. It helps prevent overfitting by penalizing large weight values.\n",
    "    *   `max_grad_norm=1.0`:\n",
    "        * During training, if the overall size (norm) of the gradients exceeds 1.0, they will be \"clipped\" (scaled down) to this maximum value. This helps prevent \"exploding gradients,\" a problem where gradients become too large and destabilize training.\n",
    "    *   `lr_scheduler_type=\"cosine\"`:\n",
    "        * This determines how the learning rate changes over the course of training. A \"cosine\" scheduler starts with the initial `learning_rate`, gradually decreases it following a cosine curve, and often reaches a very small value by the end of training. This can lead to better convergence and more robust training compared to a fixed learning rate.\n",
    "    *   `warmup_ratio=0.1`:\n",
    "        * For the first 10% (`0.1`) of the total training steps, the learning rate will gradually increase from 0 up to the main `learning_rate` (1e-4). This is called a warm-up phase.\n",
    "        * Starting with a very low learning rate and warming up can help stabilize training in the early stages when the model is making large adjustments.\n",
    "    *   `gradient_checkpointing=True`:\n",
    "        * This is a powerful memory-saving technique. Instead of storing all intermediate activations (values computed during the forward pass) needed for the backward pass, gradient checkpointing recomputes them on the fly during the backward pass. There is a trade-off, however, to keep into consideration. It saves a significant amount of GPU memory, allowing us to train larger models or use larger batch sizes/sequence lengths than would otherwise be possible. The cost is a slight increase in training time because of the recomputations.\n",
    "        * You might also see `use_reentrant=False` paired with this, which is often recommended for newer PyTorch versions for better compatibility and sometimes performance with gradient checkpointing.\n",
    "    *   `eval_strategy=\"epoch\"`, `save_strategy=\"epoch\"`:\n",
    "        * We're telling the trainer to perform an evaluation run (on a validation dataset, if provided) and to save a model checkpoint at the end of each training epoch.\n",
    "    *   `load_best_model_at_end=True`:\n",
    "        *  After all training epochs are complete, the trainer will automatically load the weights from the checkpoint that achieved the best performance on the evaluation metric.\n",
    "    *   `metric_for_best_model=\"eval_loss\"`:\n",
    "        * This specifies which metric the trainer should monitor to determine the \"best\" model. In this case, it's `eval_loss` (evaluation loss), where a lower loss is better.\n",
    "    *   `logging_steps=5`:\n",
    "        * The trainer will print out training metrics (like the current training loss) every 5 steps. This helps us monitor progress.\n",
    "    *   `report_to=\"tensorboard\"`:\n",
    "        * This tells the trainer to format its logs so they can be visualized with TensorBoard, a popular tool for inspecting training runs.\n",
    "    *   `push_to_hub=False`:\n",
    "        * By default, the model won't be automatically uploaded to the Hugging Face Hub after training. We might choose to do this manually later. If set to `True`, it would attempt an automatic upload.\n",
    "\n",
    "Finally, we have a couple of flags related to numerical precision during training:\n",
    "\n",
    "*   `fp16=False`, `bf16=True`:\n",
    "    * These flags control mixed-precision training.\n",
    "        *   `fp16` (Float16) uses 16-bit floating-point numbers, which can speed up training and reduce memory, but can sometimes lead to numerical instability (like underflow, where numbers become too small).\n",
    "        *   `bf16` (BFloat16) is another 16-bit format that has a wider dynamic range than `fp16` (similar to 32-bit floats) but less precision. It generally offers a better balance between speed/memory savings and numerical stability, especially for training large language models.\n",
    "    * We're enabling `bf16` and disabling `fp16`. This is a good choice because Gemma models are BFloat16 and our GPU supports `bf16` (typically NVIDIA Ampere architecture GPUs like A100s, or newer). It often leads to more stable training than `fp16` while still providing significant speedups and memory reduction compared to full 32-bit precision.\n",
    "\n",
    "---\n",
    "\n",
    "That's quite a list, but each of these parameters plays an important role in shaping our fine-tuning experiment. By carefully setting them up in our `Config` class, we gain fine-grained control over the process and make our experiments reproducible and easy to modify. With this configuration in place, we're almost ready to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "h_bQRd22d-CR"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    model_name = \"google/gemma-3-1b-it\"\n",
    "    dataset_name = \"lmassaron/hermes-function-calling-v1\"\n",
    "    output_dir = \"gemma-3-1b-it-function_calling\"\n",
    "    username = \"lmassaron\"\n",
    "    lora_arguments = {\n",
    "        \"r\": 16,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"target_modules\": [\n",
    "            \"embed_tokens\",\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "            \"o_proj\",\n",
    "            \"lm_head\",\n",
    "        ],\n",
    "    }\n",
    "    training_arguments = {\n",
    "        # Basic training configuration\n",
    "        \"num_train_epochs\": 1,\n",
    "        \"max_steps\": -1,\n",
    "        \"per_device_train_batch_size\": 1,\n",
    "        \"per_device_eval_batch_size\": 1,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"max_length\": 4096,\n",
    "        \"packing\": False,\n",
    "        # Optimization settings\n",
    "        \"optim\": \"adamw_torch_fused\",\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"lr_scheduler_type\": \"cosine\",\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        # Memory optimization\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
    "        # Evaluation and saving\n",
    "        \"eval_strategy\": \"epoch\",\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"save_total_limit\": 2,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"eval_loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        # Logging and output\n",
    "        \"logging_steps\": 5,\n",
    "        \"report_to\": \"tensorboard\",\n",
    "        \"logging_dir\": \"logs/runs\",\n",
    "        \"overwrite_output_dir\": True,\n",
    "        # Model sharing\n",
    "        \"push_to_hub\": True,\n",
    "        \"hub_private_repo\": False,\n",
    "    }\n",
    "    fp16 = False\n",
    "    bf16 = True\n",
    "    batch_size = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csj0jRiafPxI"
   },
   "source": [
    "The following cell creates an instance of the `Config` class and sets up the computation data type and device.\n",
    "\n",
    "- `config = Config()`: Creates an object `config` holding all the settings defined in the `Config` class.\n",
    "- `compute_dtype = torch.bfloat16`: Sets the desired data type for model computations based on the configuration (`bf16=True`). `bfloat16` offers memory savings and faster computation on compatible hardware compared to `float32`.\n",
    "- `device = \"cuda\"`: Explicitly sets the target device for computation to \"cuda\" (GPU). Assumes a CUDA-enabled GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_device():\n",
    "    \"\"\"Determine and return the optimal PyTorch device based on availability.\"\"\"\n",
    "\n",
    "    print(f\"PyTorch version: {torch.__version__}\", end=\" -- \")\n",
    "\n",
    "    # Check if MPS (Metal Performance Shaders) is available for macOS\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        print(\"using MPS device on macOS\")\n",
    "        return torch.device(\"mps\")\n",
    "\n",
    "    # Check for CUDA availability\n",
    "    detected_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"using {detected_device}\")\n",
    "    return detected_device\n",
    "\n",
    "\n",
    "def determine_compute_dtype(config):\n",
    "    \"\"\"Determine the appropriate compute dtype based on CUDA capabilities\"\"\"\n",
    "    try:\n",
    "        # Check for NVIDIA Ampere architecture (8.0) or newer\n",
    "        if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
    "            # Use bfloat16 for better training stability on newer GPUs\n",
    "            compute_dtype = torch.bfloat16\n",
    "            config.fp16 = False\n",
    "            config.bf16 = True\n",
    "        else:\n",
    "            # Fall back to float16 for older GPUs\n",
    "            compute_dtype = torch.float16\n",
    "            if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "                config.fp16 = False\n",
    "            else:\n",
    "                config.fp16 = True\n",
    "            config.bf16 = False\n",
    "    except (RuntimeError, AttributeError, IndexError) as e:\n",
    "        # Handle exceptions if CUDA is not available and you are using CPU or MPS\n",
    "        print(f\"Error determining compute dtype: {e}\")  # Log the exception\n",
    "        compute_dtype = torch.float16\n",
    "        config.fp16 = False\n",
    "        config.bf16 = False\n",
    "\n",
    "    return compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jHxefeijd-CR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute dtype: torch.bfloat16\n",
      "PyTorch version: 2.8.0+cu128 -- using cuda\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "\n",
    "# Determine optimal computation dtype based on GPU capability\n",
    "compute_dtype = determine_compute_dtype(config)\n",
    "print(\"compute dtype:\", compute_dtype)\n",
    "\n",
    "# Select the best available device (CPU, CUDA, or MPS)\n",
    "device = define_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYMNzAj5fYLI"
   },
   "source": [
    "This cell loads the tokenizer associated with the specified base model and configures it with the custom special tokens.\n",
    "\n",
    "- `AutoTokenizer.from_pretrained(config.model_name, ...)`: Loads the tokenizer corresponding to the `google/gemma-3-1b-it` model.\n",
    "- `pad_token=ChatmlSpecialTokens.pad_token.value`: Explicitly sets the padding token to `<pad>` as defined in the `ChatmlSpecialTokens` enum. This ensures consistency, especially important if the base model doesn't have a pad token or uses a different one.\n",
    "- `additional_special_tokens=ChatmlSpecialTokens.list()`: Adds all the custom tokens defined in `ChatmlSpecialTokens` (like `<tools>`, `<think>`, etc.) to the tokenizer's vocabulary. This is crucial so the tokenizer recognizes these tokens and assigns them unique IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4h3xIkqJd-CR"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_name,\n",
    "    pad_token=ChatmlSpecialTokens.pad_token.value,\n",
    "    additional_special_tokens=ChatmlSpecialTokens.list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ye8HUB2Gfxfw"
   },
   "source": [
    "This cell defines the chat template used by the tokenizer to format conversational data.\n",
    "\n",
    "Imagine you have a conversation like this:\n",
    "\n",
    "*   **User:** \"What's the weather like in London?\"\n",
    "*   **Assistant:** \"Let me check that for you.\"\n",
    "*   *(Assistant uses a tool to get weather information)*\n",
    "*   **Assistant:** \"The weather in London is sunny with a high of 20°C.\"\n",
    "\n",
    "Our model doesn't inherently understand this back-and-forth structure. We need to convert this list of messages (each with a 'role' like 'user' or 'assistant', and 'content' which is the actual text) into a single, continuous string of text that the model can process.\n",
    "\n",
    "A chat template does exactly this because it dictates how a list of messages (each with a 'role' like 'user', 'assistant' and 'content') is converted into a single string that the model can process. This formatting includes adding special control tokens (like start/end of turn markers, EOS tokens) that the model was trained to recognize.\n",
    "\n",
    "Let's take a closer look at the specific chat template we'll be using for our Gemma model, breaking down the structure step by step:\n",
    "\n",
    "- **Template Structure:**\n",
    "   - `{{ bos_token }}`: Adds the beginning-of-sequence token at the start.\n",
    "   - `{% for message in messages %}`: Iterates through the messages in the conversation.\n",
    "   - `{% if message['role'] != 'system' %}`: This specific template skips messages with the 'system' role.\n",
    "   - `{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}`: For non-system messages, it formats them using Gemma's instruction-following format:\n",
    "     - `<start_of_turn>`: Marks the beginning of a turn.\n",
    "     - `message['role']`: Includes the role (e.g., 'user', 'assistant', 'tool').\n",
    "     - `\\n`: Newline.\n",
    "     - `message['content'] | trim`: The actual message content, with leading/trailing whitespace removed.\n",
    "     - `<end_of_turn>`: Marks the end of the turn.\n",
    "     - `<eos>`: Adds an end-of-sequence token after each turn.\n",
    "     - `\\n`: Newline.\n",
    "   - `{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}`: If requested during generation, adds the prompt for the model's turn.\n",
    "\n",
    "Using the correct chat template is not just a nice-to-have; **it's critical for effective instruction following and conversational ability.** The model has learned to expect input formatted in a very specific way, including all these special tokens.\n",
    "\n",
    "If you use a template that doesn't match how the model (like Gemma here, with its instruction tuning) was trained or fine-tuned, it can get confused. It might not understand when a turn ends, who is speaking, or even that it's supposed to follow an instruction.\n",
    "\n",
    "When we get to function calling, the details of the function calls (like the tool name, arguments, or the tool's response) will simply be part of the `message['content']`. So, by defining and applying this chat template correctly, we ensure our model receives conversational data in a format it understands perfectly, setting it up for successful fine-tuning and subsequent interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wW3HzsZpfvDc"
   },
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"{{ bos_token }}{% for message in messages %}{% if message['role'] != 'system' %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfqX9OTDf_Hk"
   },
   "source": [
    "This cell loads the pre-trained causal language model specified in the configuration.\n",
    "\n",
    " - `AutoModelForCausalLM.from_pretrained(config.model_name, ...)`: Loads the `google/gemma-3-1b-it` model weights and architecture.\n",
    " - `torch_dtype=compute_dtype`: Loads the model weights using the specified data type (`torch.bfloat16`). This reduces memory footprint and potentially speeds up computation on compatible hardware.\n",
    " - `attn_implementation=\"eager\"`: Specifies the attention mechanism implementation. \"eager\" refers to the default PyTorch implementation. This might be explicitly set for compatibility or if optimized implementations like \"flash_attention_2\" are unavailable or cause issues.\n",
    " - `low_cpu_mem_usage=True`: Attempts to reduce peak CPU RAM usage during model loading by loading the state dictionary shard by shard. Useful for very large models.\n",
    " - `device_map=\"cpu\"`: Initially loads the model onto the CPU RAM. This is a strategy to avoid potential out-of-memory errors on the GPU if the full model doesn't fit alongside other requirements during the loading phase itself. The model will be moved to the GPU later.\n",
    "\n",
    "Once the model is loaded from its pre-trained state, there are a couple of critical adjustments we need to make before we can fine-tune it, especially since we've modified our tokenizer.\n",
    "\n",
    " - `model.resize_token_embeddings(len(tokenizer))`: Resizes the model's token embedding layer to match the tokenizer's vocabulary size. This is **essential** because new special tokens are added to the tokenizer. This ensures the model has corresponding embedding vectors for these new tokens, which can be trained.\n",
    " - `model = model.to(device)`: Moves the entire model (including the potentially resized embedding layer) from the CPU (where it was initially loaded) to the target computation device (`cuda` / GPU). This is necessary for GPU-accelerated training.\n",
    "\n",
    "After these steps, our pre-trained Gemma model is loaded into memory, its vocabulary is synchronized with our tokenizer (including our new special tokens), and it's sitting on the GPU, ready and waiting for the fine-tuning process to begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YZjjNiHjfw1o"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    dtype=compute_dtype,\n",
    "    attn_implementation=\"eager\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cpu\",\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SF-wbJ2fgjJU"
   },
   "source": [
    "This cell defines a function `preprocess_and_filter` to prepare individual dataset samples for the `SFTTrainer`.\n",
    "\n",
    "This function has two main jobs:\n",
    "\n",
    "1.  **Formatting:** It will take the raw list of messages (like \"user says this,\" \"assistant says that\") and use our special chat template (from Step 7) to turn it into a single, properly formatted string that our Gemma model can understand.\n",
    "2.  **Filtering:** It will check if the formatted conversation, once converted into tokens, is too long for our model to handle (based on the `max_seq_length` we set in our `Config`). If it's too long, we'll gently set it aside.\n",
    "\n",
    "Let's walk through how this function will work, step-by-step:\n",
    "\n",
    "Imagine our function receives a single training example, which we're calling `sample`. This `sample` is typically a Python dictionary, and the most important part it contains is a key named `\"messages\"`, which holds the list of turns in that particular conversation.\n",
    "\n",
    " - **Steps:**\n",
    "   1. Takes a `sample` (a dictionary expected to contain a \"messages\" key).\n",
    "   2. Extracts the `messages` list.\n",
    "   3. Uses `tokenizer.apply_chat_template(messages, tokenize=False)` to convert the list of message dictionaries into a single formatted string according to the template defined in step 7.\n",
    "   4. Encodes the resulting `text` into token IDs using `tokenizer.encode(text, truncation=False)`. Crucially, `truncation=False` is used here to get the *full* token length.\n",
    "   5. Checks if the number of tokens (`len(tokens)`) is less than or equal to the configured `max_seq_length` from `config.training_arguments`.\n",
    "   6. **If within limit:** Returns a dictionary `{\"text\": text}` containing the formatted string. `SFTTrainer` typically expects input data in a column named \"text\".\n",
    "   7. **If too long:** Returns `None`. This signals to the subsequent `.filter()` operation that this sample should be discarded.\n",
    "\n",
    " This preprocessing and filtering step is super important for a smooth and effective training process because it ensures that all sequences used for training fit within the model's context window (`max_seq_length`), preventing errors and avoiding unwanted truncation by the trainer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Lsv2hUagd-CR"
   },
   "outputs": [],
   "source": [
    "def preprocess_and_filter(sample):\n",
    "    \"\"\"Preprocesses and filters a sample based on token length\"\"\"\n",
    "    messages = sample[\"messages\"]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    tokens = tokenizer.encode(text, truncation=False)\n",
    "\n",
    "    if len(tokens) <= config.training_arguments[\"max_length\"]:\n",
    "        return {\"text\": text}\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "E31WumStd-CS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_and_filter at 0x75d53301dee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50faffc9d90f4e1eb7cf6e1d9e4cbf8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d308cb1db24c47e1be4adba69e20f29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = (\n",
    "    load_dataset(config.dataset_name, split=\"train\")\n",
    "    .rename_column(\"conversations\", \"messages\")\n",
    "    .map(preprocess_and_filter, remove_columns=\"messages\")\n",
    "    .filter(lambda x: x is not None, keep_in_memory=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRwTkqFv4lU9"
   },
   "source": [
    "The next cell splits the processed dataset into training, validation and testing subsets. This separation is crucial for building a model that doesn't just memorize the training data (\"overfitting\") but actually learns the underlying patterns and can generalize to new situations.\n",
    "\n",
    "- `dataset_splits = data.train_test_split(test_size=0.2, shuffle=True, seed=0)`: Takes the 'train' split of the loaded and processed `data` (assuming the original dataset had a 'train' split) and splits it further. 80% of the data is kept for training (becomes the new 'train' split), and 20% is held out for evaluation (becomes the 'test' split). A 80/20 split is a common practice. The `shuffle` and `seed` options help make the choice random and deterministic.\n",
    "\n",
    "By creating separate train, validation and test sets, the model learns from the 'train' set, and its performance on the unseen 'validation' set indicates how well it might perform on new, similar data. Finally, an holdout test set (`dataset_test`) is used then for the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jnoMXRGLd-CS"
   },
   "outputs": [],
   "source": [
    "dataset_train = data.train_test_split(test_size=0.2, shuffle=True, seed=0)\n",
    "dataset_test = load_dataset(config.dataset_name, split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrkK5JVV1ioo"
   },
   "source": [
    "We now work on a few functions that are important for evaluating the Gemma 3 baseline for function calling (just by means of a prompt) and after fine tuning. This is also extremely useful for seeing how the baseline Gemma 3 model (the one we downloaded, before any of our custom fine-tuning) responds to function-calling scenarios, perhaps just by prompting it cleverly.\n",
    "\n",
    "The first function, `generate_from_model_batch`, processes a batch of conversations (hence we can leverage all of the GPU memory if we are using a larger GPU such as A100), generates model responses for each, and returns the decoded text of these responses. This function is essential to interact with Gemma 3 in an easy and fast way. The function takes the following arguments:\n",
    "\n",
    "  - `batch_conversations`: A list of conversation objects. Each conversation is typically a list of dictionaries, where each dictionary has 'role' (e.g., 'user', 'assistant') and 'content' (the message text) keys.\n",
    "  - `model`: The pre-trained language model (a Hugging Face Transformer model, in our example Gemma 3-1b-it) that will be used for generation.\n",
    "  - `tokenizer`: The tokenizer corresponding to the `model`, used for converting text to token IDs and vice-versa.\n",
    "\n",
    "As for as internal instructions, the first thing we need to do inside our function is convert each of those structured conversations into a single, flat string that our model can directly read. This is where our chat template (which we discussed earlier) comes into play:\n",
    "\n",
    "- `prompts = [tokenizer.apply_chat_template(conv, tokenize=False) for conv in batch_conversations]`\n",
    "\n",
    "The loop takes our list of message dictionaries (conv) and uses the model's specific chat template to stitch them together into a single string. It adds all the special tokens (like <start_of_turn>, role names, etc.). Just notice that:\n",
    "\n",
    "  - `tokenizer.apply_chat_template`: This method takes a conversation (list of turns) and applies the model's specific chat template (e.g., adding special tokens for user/assistant roles, system prompts) to create a flat string.\n",
    "  - `tokenize=False`: Ensures the output is a string, not token IDs at this stage.\n",
    "\n",
    "The result, prompts, will be a list of strings, where each string is a perfectly formatted prompt ready for Gemma. Now that we have our text prompts, we need to convert them into the numerical format the model (and the GPU) expects: PyTorch tensors.\n",
    "\n",
    "We then use an instruction that tokenizes the previously formatted string prompts and prepares them as input tensors for the model:\n",
    "\n",
    "- `inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096, add_special_tokens=False).to(device)`\n",
    "\n",
    "    - `tokenizer(prompts, ...)`: Converts the list of prompt strings into token IDs.\n",
    "        - `return_tensors=\"pt\"`: Returns PyTorch tensors.\n",
    "        - `padding=True`: Pads shorter sequences in the batch with padding tokens to match the length of the longest sequence (or `max_length`).\n",
    "        - `truncation=True`: Truncates sequences that are longer than `max_length`.\n",
    "        - `max_length=4096`: Sets the maximum number of tokens for the input sequence (prompt).\n",
    "        - `add_special_tokens=False`: Assumes that `apply_chat_template` has already added any necessary special tokens (like BOS/EOS for the entire prompt, or role-specific tokens). This prevents the tokenizer from adding its default special tokens again.\n",
    "    - `.to(device)`: Moves the input tensors to the specified computation `device` (e.g., 'cuda' for GPU or 'cpu'). The `device` variable is assumed to be defined elsewhere in the scope.\n",
    "\n",
    "We then simply feed our prepared inputs to the model and ask it to generate responses. This instruction generates token sequences (responses) from the model based on the input prompts and specified generation parameters.\n",
    "\n",
    "- `outputs = model.generate(...)`\n",
    "    - `**inputs`: Unpacks the dictionary returned by the tokenizer (containing `input_ids`, `attention_mask`, etc.) as keyword arguments to the `model.generate` method.\n",
    "    - `max_new_tokens=256`: The model will generate at most 256 new tokens after the input prompt.\n",
    "    - `do_sample=True`: Enables sampling-based generation. If `False`, greedy decoding would be used.\n",
    "    - `top_p=0.95`: (top p sampling or nucleus sampling) At each step, considers the smallest set of tokens whose cumulative probability is at least 0.95. The model then samples from this set.\n",
    "    - `temperature=0.01`: Controls the randomness of the sampling. A very low temperature (like 0.01) makes the output more deterministic and less random, favoring higher probability tokens.\n",
    "    - `repetition_penalty=1.0`: A value of 1.0 means no penalty for repetition. Values > 1 penalize repeated tokens/phrases.\n",
    "    - `eos_token_id=tokenizer.eos_token_id`: Specifies the token ID that signifies the end of a sequence, so the model knows when to stop generating.\n",
    "\n",
    "The next step is to decode the output into text. Before proceeding we need to calculate the length (in number of tokens) of each original input prompt. We achieve that by re-tokenizing each prompt string (as done before creating `inputs`) to get its length.This is important for separating the generated text from the input prompt in the next step.\n",
    "\n",
    "- `prompt_lengths = [len(tokenizer(prompt)[\"input_ids\"]) for prompt in prompts]`\n",
    "\n",
    "After computing the length, we can decode the generated part of each output sequence back into human-readable text. In this way, we loop through the outputs from the model, isolate just the newly generated parts, and convert those token IDs back into human-readable text.\n",
    "\n",
    "- `generated_decoded = []`\n",
    "- `for i, output in enumerate(outputs):`\n",
    "  - `generated = tokenizer.decode(output[prompt_lengths[i]:], skip_special_tokens=False)`\n",
    "    - `output`: Each `output` from `model.generate` contains the token IDs for the *entire* sequence (original prompt + generated tokens).\n",
    "    - `output[prompt_lengths[i]:]`: Slices the `output` tensor to get only the token IDs corresponding to the *newly generated* tokens, by skipping the tokens of the original prompt.\n",
    "    - `tokenizer.decode(...)`: Converts these generated token IDs back into a string.\n",
    "    - `skip_special_tokens=False`: Special tokens (like `<|endoftext|>`, padding tokens if any were part of the generation before EOS) within the generated portion will *not* be removed from the decoded string.\n",
    "  - `generated_decoded.append(generated.strip())`\n",
    "    - `strip()`: Removes any leading or trailing whitespace from the decoded generated string.\n",
    "    - The cleaned-up generated string is added to the `generated_decoded` list.\n",
    "\n",
    "Finally, our function returns the generated_decoded list, where each string is the model-generated response corresponding to an input conversation from the batch.\n",
    "\n",
    "- `return generated_decoded`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FTSsG-z9qyS-"
   },
   "outputs": [],
   "source": [
    "def extract_last_model_turn(raw_output):\n",
    "    \"\"\"Extracts the content from the last model turn in a raw generated string\"\"\"\n",
    "    start_tag = \"<start_of_turn>\"\n",
    "    end_tag = \"<end_of_turn>\"\n",
    "\n",
    "    last_start_pos = raw_output.rfind(start_tag)\n",
    "\n",
    "    if last_start_pos == -1:\n",
    "        return raw_output.strip()\n",
    "\n",
    "    last_end_pos = raw_output.find(end_tag, last_start_pos)\n",
    "\n",
    "    content_start_pos = last_start_pos + len(start_tag)\n",
    "\n",
    "    if last_end_pos != -1:\n",
    "        content = raw_output[content_start_pos:last_end_pos]\n",
    "    else:\n",
    "        content = raw_output[content_start_pos:]\n",
    "\n",
    "    first_newline = content.find(\"\\n\")\n",
    "    if first_newline != -1:\n",
    "        content = content[first_newline + 1 :]\n",
    "\n",
    "    return content.replace(\"<eos>\", \"\").replace(\"<pad>\", \"\").strip()\n",
    "\n",
    "\n",
    "def generate_from_model_batch(batch_conversations, model, tokenizer):\n",
    "    # Ensure proper chat template application, including generation prompt\n",
    "    prompts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conv,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,  # Add generation prompt for chat models\n",
    "        )\n",
    "        for conv in batch_conversations\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=4096,\n",
    "        add_special_tokens=True,\n",
    "    ).to(model.device)  # Use model.device if 'device' is not globally defined\n",
    "\n",
    "    prompt_actual_lengths = inputs[\"attention_mask\"].sum(dim=1).tolist()\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.01,\n",
    "        repetition_penalty=1.0,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,  #  Explicitly set pad_token_id\n",
    "    )\n",
    "\n",
    "    generated_decoded = []\n",
    "    for i, output_tensor in enumerate(outputs):\n",
    "        # Use the accurate prompt_actual_lengths for slicing\n",
    "        generated_only_ids = output_tensor[prompt_actual_lengths[i] :]\n",
    "\n",
    "        # Decode without skipping special tokens here.\n",
    "        decoded_text = tokenizer.decode(generated_only_ids, skip_special_tokens=False)\n",
    "        clean_output = extract_last_model_turn(decoded_text)\n",
    "        generated_decoded.append(\n",
    "            clean_output.strip()\n",
    "        )  # Strip leading/trailing whitespace\n",
    "\n",
    "    return generated_decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09derDir1j8N"
   },
   "source": [
    "The following functions provide utilities for comparing two lists, focusing on different aspects of their similarity. The first function will help you evaluate the generated contents not function calling by comparing the bag of words of the generated answers and the ground truth (which we know are useful answers to the user).\n",
    "\n",
    "The `compute_matching_percentage` function provides a measure of how much of `list2` is \"covered\" or represented by `list1`, taking into account the frequency of duplicate items. It is useful for comparing multisets where the order of elements is not important, but their presence and frequency are significant (e.g., comparing sets of keywords or item features, where `list2` might be a reference set).\n",
    "\n",
    "- `def compute_matching_percentage(list1, list2)`: Computes the percentage of matching elements between two lists. It first checks if either list is empty, returning `0.0` if so. Then, it uses `collections.Counter` to get frequency counts of elements in both `list1` and `list2`. The number of matches is calculated by summing the minimum count of each common element found in both lists. Finally, this total number of matches is divided by the length of `list2` to determine the matching percentage.\n",
    "\n",
    "When Would We Use This?\n",
    "\n",
    "*   **Comparing Keywords:** If the model is supposed to summarize information, does its summary contain the key terms present in a reference summary?\n",
    "*   **General Answer Quality:** For non-function call responses, does the model use similar vocabulary to a known good answer?\n",
    "*   **Feature Comparison:** If you're comparing sets of features or tags, this can tell you how much overlap there is.\n",
    "\n",
    "As for the second function, it evaluates if the generated function calling matches the expected call from ground truth. It will look for the longest exact match and use that for scoring the result:\n",
    "\n",
    "\n",
    "When Would We Use This?\n",
    "\n",
    "*   **Evaluating Function Call Syntax:** This is perfect! We can tokenize the model's generated function call and the expected function call. This function will tell us the length of the longest part they got *exactly right in the correct order*. A high score here means the model is very close to or perfectly generating the function call.\n",
    "*   **Detecting Plagiarism:** If you split two documents into sequences of words, this could find the longest identical phrase.\n",
    "*   **Sequence Analysis:** In bioinformatics, it's used to find shared segments in DNA or protein sequences.\n",
    "\n",
    "This second function is useful for determining the extent of exact, ordered similarity between two sequences. Unlike `compute_matching_percentage` which looks at overall element overlap, this function focuses specifically on identical, uninterrupted blocks of elements. This is applicable in scenarios such as comparing sequences of events, detecting plagiarism by comparing sequences of words or characters, or analyzing genetic sequences for significant shared contiguous segments.\n",
    "\n",
    "- `def find_longest_common_sequence_length(list1, list2)`: Finds the length of the longest common *contiguous* sequence between two lists. If either input list is empty, it returns `0`. The function employs a dynamic programming approach, using `prev_row` and `current_row` to store lengths of common sequences ending at the current positions, which optimizes space. It iterates through `list1` and `list2`; if elements at the current positions match, the length of the common sequence (`current_row[j]`) is incremented based on the previous diagonal value (`prev_row[j-1] + 1`). If they don't match, the contiguous sequence is broken, and `current_row[j]` is set to `0`. The `max_length` variable keeps track of the longest sequence found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4V3kcka2EEdw"
   },
   "outputs": [],
   "source": [
    "def compute_matching_percentage(list1, list2):\n",
    "    \"\"\"Computes the percentage of matching elements between two lists.\"\"\"\n",
    "    if not list1 or not list2:\n",
    "        return 0.0\n",
    "    count1, count2 = Counter(list1), Counter(list2)\n",
    "    matches = sum(min(count1[code], count2[code]) for code in count1 if code in count2)\n",
    "    return matches / len(list2)\n",
    "\n",
    "\n",
    "def find_longest_common_sequence_length(list1, list2):\n",
    "    \"\"\"Finds the length of the longest common contiguous sequence between two lists.\"\"\"\n",
    "    if not list1 or not list2:\n",
    "        return 0\n",
    "    m, n = len(list1), len(list2)\n",
    "    prev_row = [0] * (n + 1)\n",
    "    current_row = [0] * (n + 1)\n",
    "    max_length = 0\n",
    "    for i in range(1, m + 1):\n",
    "        prev_row, current_row = current_row, prev_row\n",
    "        for j in range(1, n + 1):\n",
    "            if list1[i - 1] == list2[j - 1]:\n",
    "                current_row[j] = prev_row[j - 1] + 1\n",
    "                max_length = max(max_length, current_row[j])\n",
    "            else:\n",
    "                current_row[j] = 0\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpgFHXiU1k0k"
   },
   "source": [
    "Now it is the time for the `evaluate_function_calling` function, which wraps-up all the previous functions into an evaluation procedure.\n",
    "\n",
    "We've spent a lot of time preparing our data, configuring our model, and maybe even fine-tuning it. Now comes the crucial part: how do we *actually measure* if it's doing a good job? We need a robust way to test its ability to:\n",
    "\n",
    "1.  **Make correct tool calls** when it's supposed to.\n",
    "2.  **Provide useful, relevant answers** when no tool call is needed.\n",
    "\n",
    "This is exactly what our `evaluate_function_calling` function is designed to do. It will take a dataset of conversations (with known \"correct\" model responses), have our model generate its own responses, and then compare the two using the helper metrics we discussed earlier (`find_longest_common_sequence_length` for tool calls and `compute_matching_percentage` for general helpfulness).\n",
    "\n",
    "This function is like the \"final exam\" for our model, checking how well it handles both making tool calls and just being a helpful conversationalist.\n",
    "\n",
    "- `def evaluate_function_calling(dataset, model, tokenizer, batch_size=8):`\n",
    "    - **Arguments:**\n",
    "        - `dataset`: A list of conversation examples. Each example is expected to be a dictionary with a \"conversations\" key, which holds a list of dialogue turns (each turn being a dictionary with \"role\" and \"content\").\n",
    "        - `model`: The pre-trained language model to be evaluated.\n",
    "        - `tokenizer`: The tokenizer corresponding to the `model`.\n",
    "        - `batch_size=8`: The number of conversation queries to process in a single batch during generation.\n",
    "\n",
    "first, the function stores the total number of conversation examples in the provided `dataset`.\n",
    "\n",
    "- `test_examples = len(dataset)`\n",
    "\n",
    "The next step is to initialize empty lists to store evaluation metrics and intermediate data.\n",
    "\n",
    "- `tooling = []`, `being_useful = []`, `queries = []`, `answers = []`\n",
    "        - `tooling`: Will store match scores for responses where a tool call was expected.\n",
    "        - `being_useful`: Will store match scores for responses where a general helpful answer was expected (no tool call).\n",
    "        - `queries`: Will store the input prompts (conversation history up to the point where the model should respond).\n",
    "        - `answers`: Will store the ground truth (expected) model responses.\n",
    "\n",
    "a loop will go through each conversation example in the `dataset`.\n",
    "\n",
    "- `for i in range(test_examples): ...`\n",
    "\n",
    "For each example, we initialize an empty list to accumulate the turns of the current conversation history that will form the prompt.\n",
    "\n",
    "- `conversations = []`\n",
    "\n",
    "Inside the first loop, we furthermore loop through each turn within the current conversation example.\n",
    "\n",
    "- `for item in dataset[i][\"conversations\"]:`\n",
    "\n",
    "If the current turn is not from the \"model\" (e.g., \"user\", \"system\"), it's part of the input history. Hence, we append it to the `conversations` list that forms the prompt.\n",
    "\n",
    "- `if item[\"role\"] != \"model\": conversations.append(item)`\n",
    "\n",
    "When a \"model\" turn is encountered, it means we have a complete prompt (the `conversations` accumulated so far) and a ground truth answer.\n",
    "\n",
    "- `if item[\"role\"] == \"model\":`\n",
    "    - `queries.append(conversations[:])`: Appends a *copy* of the current `conversations` (the prompt) to the `queries` list.\n",
    "    - `answers.append(item[\"content\"])`: Appends the actual content of the model's turn (the ground truth response) to the `answers` list.\n",
    "    - `conversations.append(item)`: Appends the current model's turn to `conversations`. This is important so that if the conversation continues with more user/model turns, this model response becomes part of the history for subsequent prompts within the same example.\n",
    "\n",
    "Groups the collected `queries` into smaller `batches` of the specified `batch_size` for efficient processing by the model.\n",
    "\n",
    "- `batches = [queries[i:i + batch_size] for i in range(0, len(queries), batch_size)]`\n",
    "\n",
    "We then initialize an empty list to store the responses generated by the model.\n",
    "\n",
    "- `generated = []`\n",
    "\n",
    "We iterate through each batch of `queries` (using `tqdm` for a progress bar) and generates model responses.\n",
    "\n",
    "- `for batch in tqdm(batches): generated.extend(generate_from_model_batch(batch, model, tokenizer))`\n",
    "    - `generate_from_model_batch(batch, model, tokenizer)`: Calls a separate function (presumably defined elsewhere, as in your previous example) to get model generations for the current `batch` of prompts.\n",
    "    - `.extend()`: Adds all generated responses from the current batch to the main `generated` list.\n",
    "\n",
    "We then iterate simultaneously through the list of `answers` (ground truth) and the list of `generated` responses from the model. `zip` pairs corresponding items.\n",
    "\n",
    "- `for ground_truth, generated_response in zip(answers, generated):`\n",
    "\n",
    "We tokenize both the ground truth string and the model-generated string into sequences of token IDs. This is done to compare them at a token level.\n",
    "\n",
    "- `ground_truth_tokens = tokenizer(ground_truth)[\"input_ids\"]`\n",
    "- `generated_tokens = tokenizer(generated_response)[\"input_ids\"]`\n",
    "\n",
    "At this point, we check if the ground truth response was intended to be a tool call (signified by the presence of the `\"<tool_call>\"` string).\n",
    "\n",
    "- `if \"<tool_call>\" in ground_truth:`\n",
    "    - `seq = find_longest_common_sequence_length(ground_truth_tokens, generated_tokens)`: Calls a helper function `find_longest_common_sequence_length` (assumed to be defined elsewhere) to find the length of the longest common subsequence between the token IDs of the ground truth and the generated response.\n",
    "    - `matches = seq / len(ground_truth_tokens)`: Calculates a match score as the ratio of the longest common subsequence length to the total length of the ground truth tokens. This gives a measure of how much of the expected tool call was correctly generated.\n",
    "    - `tooling.append(matches)`: Appends this match score to the `tooling` list.\n",
    "\n",
    "If the ground truth response was *not* a tool call, it's evaluated as a general helpful exchange.\n",
    "\n",
    "- `else:`\n",
    "    - `matches = compute_matching_percentage(ground_truth_tokens, generated_tokens)`: Calls another helper function `compute_matching_percentage` (assumed to be defined elsewhere) to calculate a match score between the ground truth and generated tokens. This could be similar to LCS or another metric like ROUGE-L, or a custom token overlap.\n",
    "    - `being_useful.append(matches)`: Appends this match score to the `being_useful` list.\n",
    "\n",
    "Finally, we calculate and print the final evaluation metrics.\n",
    "\n",
    "- `print(f\"\\nAccuracy in function calling: {np.mean(tooling):0.5f}\")`\n",
    "- `print(f\"Match in helpful exchange: {np.mean(being_useful):0.5f}\")`\n",
    "    - `np.mean(tooling)`: Computes the average of all match scores for tool call responses.\n",
    "    - `np.mean(being_useful)`: Computes the average of all match scores for non-tool call (helpful) responses.\n",
    "    - `:0.5f`: Formats the output to display as a float with 5 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hs7PU1aWqy7T"
   },
   "outputs": [],
   "source": [
    "def evaluate_function_calling(dataset, model, tokenizer, batch_size=8):\n",
    "    \"\"\"Evaluates a model on a function-calling dataset\"\"\"\n",
    "    # Suppress the warning by setting the pad_token_id\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    test_examples = len(dataset)\n",
    "    tooling = []\n",
    "    being_useful = []\n",
    "    queries = []\n",
    "    answers = []\n",
    "\n",
    "    prompts_list = []\n",
    "    returned_outputs_list = []\n",
    "    expected_outputs_list = []\n",
    "    tool_call_flag_list = []\n",
    "\n",
    "    for i in range(test_examples):\n",
    "        conversations = []\n",
    "        for item in dataset[i][\"conversations\"]:\n",
    "            if item[\"role\"] != \"model\":\n",
    "                conversations.append(item)\n",
    "            if item[\"role\"] == \"model\":\n",
    "                queries.append(conversations[:])\n",
    "                answers.append(item[\"content\"])\n",
    "                conversations.append(item)\n",
    "\n",
    "    batches = [queries[i : i + batch_size] for i in range(0, len(queries), batch_size)]\n",
    "    generated_outputs = []\n",
    "    for batch in tqdm(batches):\n",
    "        # Assuming generate_from_model_batch is defined elsewhere\n",
    "        generated_outputs.extend(generate_from_model_batch(batch, model, tokenizer))\n",
    "\n",
    "    for i, (ground_truth, generated) in enumerate(zip(answers, generated_outputs)):\n",
    "        ground_truth_tokens = tokenizer(ground_truth)[\"input_ids\"]\n",
    "        generated_tokens = tokenizer(generated)[\"input_ids\"]\n",
    "\n",
    "        is_tool_call = \"<tool_call>\" in ground_truth\n",
    "        tool_call_flag_list.append(is_tool_call)\n",
    "        prompts_list.append(\n",
    "            tokenizer.decode(tokenizer(str(queries[i]))[\"input_ids\"])\n",
    "        )  # This remains the same\n",
    "        returned_outputs_list.append(generated)\n",
    "        expected_outputs_list.append(ground_truth)\n",
    "\n",
    "        if is_tool_call:\n",
    "            seq = find_longest_common_sequence_length(\n",
    "                ground_truth_tokens, generated_tokens\n",
    "            )\n",
    "            matches = (\n",
    "                seq / len(ground_truth_tokens) if len(ground_truth_tokens) > 0 else 0\n",
    "            )\n",
    "            tooling.append(matches)\n",
    "        else:\n",
    "            matches = compute_matching_percentage(ground_truth_tokens, generated_tokens)\n",
    "            being_useful.append(matches)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\n",
    "        f\"\\nAccuracy in function calling: {np.mean(tooling) if tooling else 0.0:0.5f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Match in helpful exchange: {np.mean(being_useful) if being_useful else 0.0:0.5f}\"\n",
    "    )\n",
    "\n",
    "    results_df = pd.DataFrame(\n",
    "        {\n",
    "            \"prompt\": prompts_list,\n",
    "            \"returned_output\": returned_outputs_list,\n",
    "            \"expected_output\": expected_outputs_list,\n",
    "            \"tool_call\": tool_call_flag_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FyWzUG1k-U4Y"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453d7ae56cb74f66a0cb7fafad18f89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy in function calling: 0.27905\n",
      "Match in helpful exchange: 0.28897\n"
     ]
    }
   ],
   "source": [
    "results_dataframe = evaluate_function_calling(\n",
    "    dataset_test.select(range(300)), model, tokenizer, batch_size=config.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vB1RPeLM5jI6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a208b04219c430e8a12bf0a45f56b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88689b74f15a49199605b1926c50306c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e772c1186e574439a20f255f2674ad12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c1b4d1e3fd4176838139a514a0e993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1356e9e3d7d4455ad06bb8b0048e415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        : 100%|##########|  722kB /  722kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f1af724f0c4123a7b551c1d7cf08fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    }
   ],
   "source": [
    "if config.training_arguments[\"push_to_hub\"]:\n",
    "    username = config.username\n",
    "    repo_name = \"gemma-3-1b-it-function_calling-eval-noft\"\n",
    "    evaluation_dataset = Dataset.from_pandas(results_dataframe)\n",
    "    evaluation_dataset.push_to_hub(f\"{username}/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsU8VLXJ4qLh"
   },
   "source": [
    "This cell sets up the configuration for Parameter-Efficient Fine-Tuning (PEFT) using the LoRA technique.\n",
    "\n",
    "The `peft_config` object contains all the necessary information for the `SFTTrainer` to modify the base model by injecting LoRA adapters according to the specified parameters.\n",
    "\n",
    "- `LoraConfig(...)`: Creates an instance of the LoRA configuration class.\n",
    "- `**config.lora_arguments`: Unpacks the dictionary of LoRA-specific hyperparameters (`r`, `lora_alpha`, `lora_dropout`, `target_modules`) defined earlier in the main `Config` class.\n",
    "- `task_type=TaskType.CAUSAL_LM`: Explicitly specifies that the PEFT technique (LoRA) is being applied to a Causal Language Model. This helps `peft` configure the model adaptation correctly for generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nok_J5xnd-CS"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    **config.lora_arguments,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2EhP4H35aM2"
   },
   "source": [
    "This cell initializes the configuration object specifically required by the `SFTTrainer`.\n",
    "\n",
    "This `training_arguments` object gathers all settings related to the training loop itself (optimization, scheduling, evaluation, saving, logging, etc.) into the format expected by the `SFTTrainer`.\n",
    "\n",
    " - `SFTConfig(...)`: Creates an instance of `SFTConfig`, which is a subclass of `transformers.TrainingArguments` tailored for the `SFTTrainer`.\n",
    " - `**config.training_arguments`: Unpacks the dictionary of general training hyperparameters (learning rate, batch size, epochs, optimization settings, logging, saving strategies, etc.) defined in the main `Config` class.\n",
    " - `output_dir=config.output_dir`: Explicitly sets the output directory where checkpoints and logs will be saved.\n",
    " - `fp16=config.fp16`, `bf16=config.bf16`: Sets the mixed-precision training flags based on the main configuration.\n",
    "\n",
    "In addition we have settings that optimize the model configuration for the training phase, primarily focusing on memory efficiency (`use_cache=False`) and potential compatibility (`pretraining_tp=1`).:\n",
    "\n",
    "- `model.config.use_cache = False`: Disables the Key/Value (KV) cache mechanism in the model's attention layers. The KV cache speeds up *inference* by reusing past computations, but it's not needed during *training* and consumes significant GPU memory. Disabling it frees up memory, which is often crucial, especially when using gradient checkpointing.\n",
    "- `model.config.pretraining_tp = 1`: Sets the `pretraining_tp` (tensor parallelism used during pre-training) value to 1. This setting can sometimes be necessary for compatibility when fine-tuning models that were originally pre-trained with tensor parallelism, especially if the fine-tuning setup doesn't use the same degree of parallelism. Setting it to 1 essentially tells the configuration not to expect weights sharded in a particular way due to pre-training parallelism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9VM-RRw55CK8"
   },
   "outputs": [],
   "source": [
    "training_arguments = SFTConfig(\n",
    "    **config.training_arguments,\n",
    "    output_dir=config.output_dir,\n",
    "    fp16=config.fp16,\n",
    "    bf16=config.bf16,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCGlHD-Q5qpD"
   },
   "source": [
    "This cell creates the `SFTTrainer` instance, which will manage the fine-tuning process. The `SFTTrainer` object encapsulates the model, data, tokenizer, and all configurations needed to run the supervised fine-tuning loop, handle evaluation, checkpointing, and logging.\n",
    "\n",
    "- `SFTTrainer(...)`: Initializes the trainer class from the `trl` library.\n",
    "- **Arguments:**\n",
    "   - `model=model`: The language model to be fine-tuned. The `peft` library will automatically modify this model based on `peft_config` when training starts.\n",
    "   - `args=training_arguments`: The `SFTConfig` object containing all training hyperparameters and settings.\n",
    "   - `train_dataset=dataset[\"train\"]`: The dataset split to be used for training.\n",
    "   - `eval_dataset=dataset[\"test\"]`: The dataset split to be used for evaluation.\n",
    "   - `tokenizer=tokenizer`: The tokenizer to be used for processing data (though much preprocessing was done manually here, the trainer might use it for collation or other internal steps). `processing_class` seems like a typo and likely should be `tokenizer`. Assuming it means `tokenizer`.\n",
    "   - `peft_config=peft_config`: The `LoraConfig` object specifying how LoRA should be applied. Passing this instructs the trainer to use PEFT.\n",
    "\n",
    "Then the cell initiates the actual fine-tuning process.\n",
    "\n",
    "This starts the training loop. The trainer will:\n",
    "   - Apply the LoRA modifications to the model based on `peft_config`.\n",
    "   - Iterate through the `train_dataset` for the specified number of epochs/steps.\n",
    "   - Compute loss, perform backpropagation, and update the LoRA adapter weights (and any other trainable parameters like embeddings).\n",
    "   - Perform evaluation on the `eval_dataset` based on the `eval_strategy`.\n",
    "   - Save model checkpoints based on the `save_strategy`.\n",
    "   - Log metrics according to `logging_steps` and `report_to`.\n",
    "   - Finally, load the best checkpoint if `load_best_model_at_end=True`.\n",
    "\n",
    " - `trainer.train()`: Calls the `train` method of the `SFTTrainer` instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "RS4QydkKd-CT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmassaron/code/function-calling/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:693: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.embed_tokens', 'lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb014442d30a47b3940283359b9d6f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/3332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33082b9e3e384d73a9f6271106ea1d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/3332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c826406dcc604258a2b950288f97fae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/3332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581632ea35d14ed593ced859fe42f681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35183c3e89be47baa1cf58a85954227f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7890da987ee84ecbaa129a6ac5bf4fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='833' max='833' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [833/833 14:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.194100</td>\n",
       "      <td>0.227374</td>\n",
       "      <td>0.222978</td>\n",
       "      <td>2303839.000000</td>\n",
       "      <td>0.934002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmassaron/code/function-calling/.venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:270: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=833, training_loss=0.3769577017494467, metrics={'train_runtime': 888.3272, 'train_samples_per_second': 3.751, 'train_steps_per_second': 0.938, 'total_flos': 9943842572129856.0, 'train_loss': 0.3769577017494467, 'epoch': 1.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset_train[\"train\"],\n",
    "    eval_dataset=dataset_train[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nioMHvp96Cel"
   },
   "source": [
    "The next cell saves the results of the fine-tuning process locally, helping persisting the fine-tuning results (adapter weights) and the corresponding tokenizer configuration needed to correctly load and use the fine-tuned model later for inference. Saving them together ensures compatibility.\n",
    "\n",
    " - `trainer.model.save_pretrained(\"LoRA_\" + config.output_dir, save_embedding_layers=True)`: Saves the trained PEFT adapter weights (the LoRA layers) to a directory named \"LoRA_gemma-3-1b-it-function_calling\". Because LoRA was used, this saves only the small adapter weights, not the entire base model. `save_embedding_layers=True` attempts to save the fine-tuned input/output embedding layers if they were targeted by LoRA or resized and made trainable; this behavior can vary across library versions.\n",
    " - `tokenizer.eos_token = \"<eos>\"`: Explicitly sets the `eos_token` attribute of the tokenizer object. This might be redundant if already configured but acts as a safeguard.\n",
    " - `tokenizer.save_pretrained(\"LoRA_\" + config.output_dir)`: Saves the tokenizer's configuration (including vocabulary, added special tokens like the ChatML ones, and the custom chat template) to the same directory as the LoRA adapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0WEsyn5ld-CT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LoRA_gemma-3-1b-it-function_calling/tokenizer_config.json',\n",
       " 'LoRA_gemma-3-1b-it-function_calling/special_tokens_map.json',\n",
       " 'LoRA_gemma-3-1b-it-function_calling/chat_template.jinja',\n",
       " 'LoRA_gemma-3-1b-it-function_calling/tokenizer.model',\n",
       " 'LoRA_gemma-3-1b-it-function_calling/added_tokens.json',\n",
       " 'LoRA_gemma-3-1b-it-function_calling/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving LoRA weights and tokenizer\n",
    "trainer.model.save_pretrained(\"LoRA_\" + config.output_dir, save_embedding_layers=True)\n",
    "tokenizer.eos_token = \"<eos>\"\n",
    "tokenizer.save_pretrained(\"LoRA_\" + config.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krHlf8oR6DFZ"
   },
   "source": [
    "This cell handles authentication with the Hugging Face Hub, using secrets management within Google Colab.\n",
    "\n",
    "First, it securely authenticates the session to allow uploading the fine-tuned adapter and tokenizer to a user's repository on the Hugging Face Hub. Using secrets avoids hardcoding sensitive tokens in the notebook. If you want to use your own secrets on Colab, too, have a look at the icon bar on the left and click on the key icon. You will be shown an interface where you can add a secret by name and relative value, decide what secrets are accessible by the notebook and furthermore manage your secrets by copying, discarding or importing them from Google AI Studio (for instance Gemini API keys).\n",
    "\n",
    " - `from huggingface_hub import login`: Imports the login function.\n",
    " - `from google.colab import userdata`: Imports the utility for accessing secrets stored in Colab.\n",
    " - `userdata.get('HF_TOKEN')`: Attempts to retrieve a secret named 'HF_TOKEN' (stored in Colab), which should contain a Hugging Face API token with write permissions.\n",
    " - `login(hf_token)`: If the token is found, this function authenticates the Colab environment with the Hugging Face Hub, allowing subsequent push operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4-JzB3gid-CT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "\n",
    "    hf_token = userdata.get(\"HF_TOKEN\")\n",
    "except:\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    login(hf_token)\n",
    "    print(\"Successfully logged in!\")\n",
    "else:\n",
    "    print(\"Token not found. Check Secrets configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B32GItfJ6TeX"
   },
   "source": [
    "This cell uploads the saved LoRA adapter weights and the tokenizer configuration to the specified repository on the Hugging Face Hub. The purpose is to make the fine-tuned LoRA adapter and its corresponding tokenizer publicly or privately accessible via the Hugging Face Hub, facilitating sharing, collaboration, and easy loading for inference elsewhere.\n",
    "\n",
    " - `username`: **Placeholder:** set your actual Hugging Face Hub username in the config class.\n",
    " - `output_dir = \"gemma-3-1b-it-function_calling\"`: This is the choosen name for the repository on the Hub. You can opt for a different name.\n",
    " - `trainer.push_to_hub(f\"{username}/{output_dir}\")`: Uploads the contents of the local directory where the adapter was saved (by `trainer.model.save_pretrained`) to the specified Hub repository (`username/output_dir`). This includes the adapter weights (`adapter_model.safetensors`) and configuration (`adapter_config.json`).\n",
    " - `tokenizer.push_to_hub(f\"{username}/{output_dir}\", token=True)`: Uploads the tokenizer files (saved by `tokenizer.save_pretrained`) to the *same* Hub repository. `token=True` ensures the authentication token is used, though it might be implicit after `login`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uEbQIkEXd-CT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmassaron/code/function-calling/.venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:270: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7965932aa3f4400b48c8e000bb240e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8418f7b1f0764854afb8dd9b5b4c95d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d91bb5c26fe423e8ebe81d824f746bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...-function_calling/training_args.bin: 100%|##########| 6.16kB / 6.16kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fddce0b30cf4558aeaaf4b577834be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...it-function_calling/tokenizer.model: 100%|##########| 4.69MB / 4.69MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a264914493548108b6572d043635406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...n_calling/adapter_model.safetensors:   8%|7         |  101MB / 1.29GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be6a8eda1b44abbb6101fe8b9115590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...-it-function_calling/tokenizer.json: 100%|##########| 33.4MB / 33.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4c8bbe638d439cba8408b567357209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159f9b5e96004cfdbbe1bb39fa49c26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e184e91fba40819ac2b685a65a8e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...it-function_calling/tokenizer.model: 100%|##########| 4.69MB / 4.69MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322dc9ed369a44b29c991a625b5ed039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...-it-function_calling/tokenizer.json: 100%|##########| 33.4MB / 33.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    }
   ],
   "source": [
    "if config.training_arguments[\"push_to_hub\"]:\n",
    "    username = config.username\n",
    "    output_dir = \"gemma-3-1b-it-function_calling\"\n",
    "    trainer.push_to_hub(f\"{username}/{output_dir}\")\n",
    "    tokenizer.push_to_hub(f\"{username}/{output_dir}\", token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjRufwbPrGzp"
   },
   "source": [
    "This final step in our tutorial is all about actually running that evaluation function. This will give us concrete numbers on how well our model performs on two key aspects:\n",
    "\n",
    "- Accuracy in Function Calling: How often does it correctly generate the tool calls when it's supposed to?\n",
    "\n",
    "- Match in Helpful Exchanges: When a tool call isn't needed, how well does its general conversational response match what we'd consider a good, helpful answer?\n",
    "\n",
    "Before proceeding with the evaluation, we remove the trainer and model and try to free the VRAM of the GPU. By reloading the model, we can avoid being limited by the gradient checkpointing settings (where no caching is possible) and operate in inference mode with caching enabled for speedier generations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "K24fLKhCnm_T"
   },
   "outputs": [],
   "source": [
    "del [trainer, model]\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "-ScDoK7_xPLc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmassaron/code/function-calling/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:693: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.embed_tokens', 'lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"LoRA_gemma-3-1b-it-function_calling\"\n",
    "peftconfig = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    peftconfig.base_model_name_or_path,\n",
    "    attn_implementation=\"eager\",\n",
    "    device_map=device,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model = model.to(torch.bfloat16)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "suwSACTO9rZz"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdc47bb3f4c47de90415b3f7799e040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy in function calling: 0.97335\n",
      "Match in helpful exchange: 0.81500\n"
     ]
    }
   ],
   "source": [
    "results_dataframe = evaluate_function_calling(\n",
    "    dataset_test.select(range(300)), model, tokenizer, batch_size=config.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "N0TdIxGk5X1X"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d4fa234e224ee0affa9ff04f54e163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd172068ab9f440bb14e8e827a1605b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f1d886fc804736b8e91c044da26a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0d0d1a56b148bb9e642e9bf552e905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9bef4c13f164726b6694b749c868843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :  64%|######3   |  395kB /  618kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f52217f393942fdaf246b832ee13dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if config.training_arguments[\"push_to_hub\"]:\n",
    "    username = config.username\n",
    "    repo_name = \"gemma-3-1b-it-function_calling-eval-ft\"\n",
    "    evaluation_dataset = Dataset.from_pandas(results_dataframe)\n",
    "    evaluation_dataset.push_to_hub(f\"{username}/{repo_name}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPdymHrhGrvjt3Pw7jM5D4t",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "function-calling (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
