{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNdoBv72vWPsS7kibqpcfr5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5b9f20992e494d7391c1a40522a576e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a899ccc4b42544cb9799c7de3447a49d",
              "IPY_MODEL_d5baf675bb964f5aabde7fcacf781dbf",
              "IPY_MODEL_f1254e05f6804295a9b1060a7f93563a"
            ],
            "layout": "IPY_MODEL_b9686804aabf45499bc5192898891e53"
          }
        },
        "a899ccc4b42544cb9799c7de3447a49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32a2f18fdee04eeca5b21159f4b3db8b",
            "placeholder": "​",
            "style": "IPY_MODEL_4d0403ea15a24a01b093bd5521d82fb1",
            "value": "Tokenizing train dataset: 100%"
          }
        },
        "d5baf675bb964f5aabde7fcacf781dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6eb6cbafcbf94e18a7773d77adaba032",
            "max": 3326,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b3b936d62874c1da9524a6418a8c1c8",
            "value": 3326
          }
        },
        "f1254e05f6804295a9b1060a7f93563a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7fec746059e400b8e87d780f4d9c343",
            "placeholder": "​",
            "style": "IPY_MODEL_ea356142db9142eeb252a0dd4c27af29",
            "value": " 3326/3326 [00:06&lt;00:00, 502.22 examples/s]"
          }
        },
        "b9686804aabf45499bc5192898891e53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32a2f18fdee04eeca5b21159f4b3db8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d0403ea15a24a01b093bd5521d82fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6eb6cbafcbf94e18a7773d77adaba032": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b3b936d62874c1da9524a6418a8c1c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7fec746059e400b8e87d780f4d9c343": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea356142db9142eeb252a0dd4c27af29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "851f299f626641d8bf531c2aef03e2d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4468a196ad146ec8ab04bd4d16a9aeb",
              "IPY_MODEL_77db871e1da9495a8f5fa8a337b518da",
              "IPY_MODEL_5725fdf8f5984c87b154906d2523ee57"
            ],
            "layout": "IPY_MODEL_2668332d53ff4a8b969f7b3af6ded37b"
          }
        },
        "b4468a196ad146ec8ab04bd4d16a9aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e87e94976c7f4fb888229309b329a48e",
            "placeholder": "​",
            "style": "IPY_MODEL_56194529b22b4b82997965a95ab2dc45",
            "value": "Packing train dataset: 100%"
          }
        },
        "77db871e1da9495a8f5fa8a337b518da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61c8bc85b48a40e38203a764cfedf14f",
            "max": 3326,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62ec77a30f634e44afaef93cda622948",
            "value": 3326
          }
        },
        "5725fdf8f5984c87b154906d2523ee57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6bc5e917eeb4eec89f8397abe25d402",
            "placeholder": "​",
            "style": "IPY_MODEL_e6b54e94e7a540c8b9d4bbddbc1864c7",
            "value": " 3326/3326 [00:00&lt;00:00, 125190.75 examples/s]"
          }
        },
        "2668332d53ff4a8b969f7b3af6ded37b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e87e94976c7f4fb888229309b329a48e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56194529b22b4b82997965a95ab2dc45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61c8bc85b48a40e38203a764cfedf14f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62ec77a30f634e44afaef93cda622948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6bc5e917eeb4eec89f8397abe25d402": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6b54e94e7a540c8b9d4bbddbc1864c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e6889d7f60644e78354f26b0cdf5a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3152186d95d4083acb80810fa9cb89e",
              "IPY_MODEL_30f49243e19841dea19eaea5d5f5f03e",
              "IPY_MODEL_6036cb433bda4198a27c5bdd37854362"
            ],
            "layout": "IPY_MODEL_b24da55610ee4c899d70c54a674558f8"
          }
        },
        "b3152186d95d4083acb80810fa9cb89e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18bcbd8ef2c34566b57bf4555753e9a7",
            "placeholder": "​",
            "style": "IPY_MODEL_0174939342a943688d4b370090934567",
            "value": "Tokenizing eval dataset: 100%"
          }
        },
        "30f49243e19841dea19eaea5d5f5f03e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0385595659e54877b8ef19aff4c49866",
            "max": 832,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b253c9d6eef64d24b6830693fd734ed4",
            "value": 832
          }
        },
        "6036cb433bda4198a27c5bdd37854362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af14a847f8a44d95b2714990934b7f73",
            "placeholder": "​",
            "style": "IPY_MODEL_6c153ce08e5d4ab18b1f5c347a0bba6c",
            "value": " 832/832 [00:01&lt;00:00, 589.99 examples/s]"
          }
        },
        "b24da55610ee4c899d70c54a674558f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18bcbd8ef2c34566b57bf4555753e9a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0174939342a943688d4b370090934567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0385595659e54877b8ef19aff4c49866": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b253c9d6eef64d24b6830693fd734ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af14a847f8a44d95b2714990934b7f73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c153ce08e5d4ab18b1f5c347a0bba6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5dbdf7a0b0043a0a8dc0e6e48ad43d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_045483cc63fb4144910024520b023611",
              "IPY_MODEL_918ab83817ba4199a69336327df30427",
              "IPY_MODEL_477a782639b44745807bb4e3a4cc2eed"
            ],
            "layout": "IPY_MODEL_15282ae0a06f40b0974dfa04d830e596"
          }
        },
        "045483cc63fb4144910024520b023611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e6c4f7d08b646c9ac6064d5b8e6f490",
            "placeholder": "​",
            "style": "IPY_MODEL_3d7714cbd5274dda96022943c91d593d",
            "value": "Packing eval dataset: 100%"
          }
        },
        "918ab83817ba4199a69336327df30427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37047688be0244a8aa6fb722fbb19c5e",
            "max": 832,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_223e32432d2e4903930b14f8e4296136",
            "value": 832
          }
        },
        "477a782639b44745807bb4e3a4cc2eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca9218b76dcc40e5acd35cd600e1d144",
            "placeholder": "​",
            "style": "IPY_MODEL_05520385b7f541728b52c29f241452fe",
            "value": " 832/832 [00:00&lt;00:00, 59486.57 examples/s]"
          }
        },
        "15282ae0a06f40b0974dfa04d830e596": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e6c4f7d08b646c9ac6064d5b8e6f490": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d7714cbd5274dda96022943c91d593d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37047688be0244a8aa6fb722fbb19c5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "223e32432d2e4903930b14f8e4296136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca9218b76dcc40e5acd35cd600e1d144": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05520385b7f541728b52c29f241452fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We start this tutorial on fine-tuning a Gemma 3 mode for function calling by installing or updating the necessary Python packages required for the process. The necessary packages for running the code are:\n",
        "\n",
        "- `transformers`: Provides access to pre-trained a large range of language models (like Gemma), tokenizers, and training utilities from Hugging Face.\n",
        "- `accelerate`: Simplifies running PyTorch code on various hardware setups (CPU, single/multi-GPU, TPU) and handles mixed-precision training.\n",
        "- `datasets`: Used for efficiently loading, processing, and manipulating datasets, especially those hosted on the Hugging Face Hub.\n",
        "- `peft`: (Parameter-Efficient Fine-Tuning) Enables techniques like LoRA (Low-Rank Adaptation) to fine-tune large models efficiently by training only a small number of extra parameters.\n",
        "- `trl`: (Transformer Reinforcement Learning library) Provides tools for fine-tuning language models, including the `SFTTrainer` used here for Supervised Fine-Tuning."
      ],
      "metadata": {
        "id": "ppSo_A50d7Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U trl"
      ],
      "metadata": {
        "id": "C6pflsoAd-CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we proceed importing the required modules and classes from the installed libraries and Python's standard library. In particular take notice about:\n",
        "\n",
        "- `os`: Used for interacting with the operating system.\n",
        "- `enum.Enum`: Used to create enumeration types, here specifically for defining special tokens in a structured way.\n",
        "- `torch`: The core PyTorch library for tensor computations and neural network modules.\n",
        "- `transformers`: Imports `AutoModelForCausalLM` (to load the language model), `AutoTokenizer` (to load the tokenizer), and `set_seed` (for reproducibility, although not used here).\n",
        "- `datasets`: Imports `load_dataset` for fetching data from the Hugging Face Hub.\n",
        "- `trl`: Imports `SFTConfig` (configuration for supervised fine-tuning) and `SFTTrainer` (the class that handles the training process).\n",
        "- `peft`: Imports `LoraConfig` (configuration for LoRA) and `TaskType` (to specify the type of task for PEFT, e.g., Causal LM)."
      ],
      "metadata": {
        "id": "2AV3ibJueKx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "from math import ceil\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
        "from datasets import load_dataset\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from peft import LoraConfig, TaskType\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "BepmFj8Pd-CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then start trying to make all the process deterministic, using the helper utility set_seed that sets the random seed for Python's random, numpy, torch (across various devices), and tensorflow (if available) to ensure reproducible results."
      ],
      "metadata": {
        "id": "9lW35X1iC0vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)"
      ],
      "metadata": {
        "id": "x408hJAk8IW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines an `Enum` class `ChatmlSpecialTokens` to manage custom special tokens related to function/tool calling within the ChatML format. A class method is provided to easily retrieve all defined special token values as a list, useful for adding them to the tokenizer.\n",
        "\n",
        "Using an Enum provides a robust and readable way to handle these additional tokens consistently throughout the script, avoiding potential typos with raw strings. We actually need new tokens in order to better handle how Gemma will refer to calling external functions and the used inputs and parameters and the tool response.\n",
        "These tokens are crucial for training the model to understand and generate text involving tool interactions, as expected by the chosen dataset (`hermes-function-calling-v1`).\n",
        "\n",
        "Here are the additional tokens that we will be using:\n",
        "\n",
        "  - `<tools>`, `</tools>`: Delimit a section describing available tools.\n",
        "  - `<think>`, `</think>`: Delimit the model's internal thought process before acting.\n",
        "  - `<tool_call>`, `</tool_call>`: Delimit the model's request to call a specific tool.\n",
        "  - `<tool_response>`, `</tool_response>`: Delimit the response received after executing a tool.\n",
        "  - `<pad>`: Padding token used to make sequences in a batch the same length.\n",
        "  - `<eos>`: End-of-sequence token, often used to signal the end of a generated turn or document."
      ],
      "metadata": {
        "id": "uRs3NrQ6edeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatmlSpecialTokens(str, Enum):\n",
        "    \"\"\"Enum class defining special tokens used in the ChatML format\"\"\"\n",
        "\n",
        "    tools = \"<tools>\"\n",
        "    eotools = \"</tools>\"\n",
        "    think = \"<think>\"\n",
        "    eothink = \"</think>\"\n",
        "    tool_call = \"<tool_call>\"\n",
        "    eotool_call = \"</tool_call>\"\n",
        "    tool_response = \"<tool_response>\"\n",
        "    eotool_response = \"</tool_response>\"\n",
        "    pad_token = \"<pad>\"\n",
        "    eos_token = \"<eos>\"\n",
        "\n",
        "    @classmethod\n",
        "    def list(cls):\n",
        "        return [c.value for c in cls]"
      ],
      "metadata": {
        "id": "ORgc0JQjd-CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell centralizes all configuration parameters for the fine-tuning script within a `Config` class.\n",
        "\n",
        " - **Purpose:** Grouping settings makes the script organized, easier to read, and simpler to modify hyperparameters.\n",
        " - **Key Parameters & Rationale:**\n",
        "   - `model_name`: \"google/gemma-3-1b-it\" - Specifies the base pre-trained model to fine-tune. Gemma-3-1B-IT is an instruction-tuned version of Google's Gemma model.\n",
        "   - `dataset_name`: \"lmassaron/hermes-function-calling-v1\" - The dataset used for fine-tuning, containing examples of conversations involving function/tool calls.\n",
        "   - `output_dir`: \"gemma-3-1B-it-function_calling\" - The directory where trained model artifacts (LoRA adapters, checkpoints) will be saved.\n",
        "   - `lora_arguments`: Configuration for LoRA (Parameter-Efficient Fine-Tuning).\n",
        "     - `r=16`: Rank of the LoRA matrices. A higher rank allows for more expressiveness but increases the number of trainable parameters. 16 is a common value offering a good balance.\n",
        "     - `lora_alpha=64`: Scaling factor for LoRA. Often set as 2x or 4x the rank (`r`). It controls the magnitude of the adaptation applied by the LoRA weights. `64` provides a strong scaling relative to `r=16`.\n",
        "     - `lora_dropout=0.05`: Dropout rate applied to LoRA layers to prevent overfitting during fine-tuning. 0.05 is a relatively low dropout rate.\n",
        "     - `target_modules`: List of modules within the base model where LoRA adapters will be injected. Targeting attention query/key/value/output projections (`q_proj`, `k_proj`, `v_proj`, `o_proj`) and feed-forward layers (`gate_proj`, `up_proj`, `down_proj`) is standard practice. Including `embed_tokens` and `lm_head` allows fine-tuning of input embeddings and the final output layer, potentially beneficial when adding special tokens or adapting to specific output formats.\n",
        "   - `training_arguments`: Configuration for the `SFTTrainer` (via `SFTConfig`, which inherits from `transformers.TrainingArguments`).\n",
        "     - `num_train_epochs=1`: The model will iterate over the entire training dataset once. Often sufficient for fine-tuning, especially with large datasets or effective techniques like LoRA, to prevent overfitting.\n",
        "     - `per_device_train_batch_size=1`: Number of samples processed per GPU per forward/backward pass during training. Kept low (1) likely due to GPU memory constraints with a large `max_seq_length`.\n",
        "     - `gradient_accumulation_steps=4`: Number of steps to accumulate gradients before performing a weight update. Simulates a larger batch size (`1 * 4 = 4`) without increasing memory usage proportionally. Helps stabilize training.\n",
        "     - `max_seq_length=2048`: Maximum token length for sequences fed into the model. Sequences longer than this will be filtered out. This value impacts memory usage significantly. 2048 is a reasonable context window for many modern models like Gemma.\n",
        "     - `packing=True`: Enables packing multiple short sequences into a single sequence up to `max_seq_length`, separated by EOS tokens. Improves training efficiency by reducing the amount of padding needed.\n",
        "     - `optim=\"adamw_torch_fused\"`: Specifies the AdamW optimizer implementation. The `_fused` version often provides better performance on GPUs.\n",
        "     - `learning_rate=1e-4`: The initial learning rate for the optimizer. `1e-4` is a common starting point for LoRA fine-tuning.\n",
        "     - `weight_decay=0.1`: Applies L2 regularization to prevent overfitting.\n",
        "     - `max_grad_norm=1.0`: Clips gradients to a maximum norm of 1.0 to prevent exploding gradients during training.\n",
        "     - `lr_scheduler_type=\"cosine\"`: Uses a cosine annealing learning rate scheduler, which gradually decreases the LR, often leading to better convergence.\n",
        "     - `warmup_ratio=0.1`: 10% of the total training steps will be used for a linear learning rate warm-up phase, starting from 0 and increasing to the `learning_rate`. Helps stabilize training early on.\n",
        "     - `gradient_checkpointing=True`: Saves significant GPU memory by recomputing activations during the backward pass instead of storing them all. Essential for training large models on limited hardware, at the cost of slightly slower training speed. `use_reentrant=False` is often recommended with newer PyTorch versions.\n",
        "     - `eval_strategy=\"epoch\"`, `save_strategy=\"epoch\"`: Perform evaluation and save model checkpoints at the end of each epoch.\n",
        "     - `load_best_model_at_end=True`: After training finishes, the trainer will load the checkpoint corresponding to the best evaluation metric.\n",
        "     - `metric_for_best_model=\"eval_loss\"`: The metric used to determine the \"best\" model (lower evaluation loss is better).\n",
        "     - `logging_steps=5`: Log training metrics (like loss) every 5 steps.\n",
        "     - `report_to=\"tensorboard\"`: Specifies that logs should be formatted for TensorBoard.\n",
        "     - `push_to_hub=False`: Whether to automatically push the model to the Hugging Face Hub after training (set to `True` later for explicit push).\n",
        "   - `fp16=False`, `bf16=True`: Configures mixed-precision training. `bf16` (BFloat16) is preferred on modern GPUs (Ampere architecture and newer) as it offers a better balance between speed/memory savings and numerical stability compared to `fp16` (Float16) for training large models."
      ],
      "metadata": {
        "id": "do7Q-E3-e0kV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    model_name = \"google/gemma-3-1b-it\"\n",
        "    dataset_name = \"lmassaron/hermes-function-calling-v1\"\n",
        "    output_dir = \"gemma-3-1B-it-function_calling\"\n",
        "    lora_arguments = {\n",
        "        \"r\": 16,\n",
        "        \"lora_alpha\": 64,\n",
        "        \"lora_dropout\": 0.05,\n",
        "        \"target_modules\": [\n",
        "            \"embed_tokens\",\n",
        "            \"q_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"gate_proj\",\n",
        "            \"up_proj\",\n",
        "            \"down_proj\",\n",
        "            \"o_proj\",\n",
        "            \"lm_head\",\n",
        "        ],\n",
        "    }\n",
        "    training_arguments = {\n",
        "        # Basic training configuration\n",
        "        \"num_train_epochs\": 1,\n",
        "        \"max_steps\": -1,\n",
        "        \"per_device_train_batch_size\": 1,\n",
        "        \"per_device_eval_batch_size\": 1,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"max_seq_length\": 2048,\n",
        "        \"packing\": True,\n",
        "        # Optimization settings\n",
        "        \"optim\": \"adamw_torch_fused\",\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"weight_decay\": 0.1,\n",
        "        \"max_grad_norm\": 1.0,\n",
        "        \"lr_scheduler_type\": \"cosine\",\n",
        "        \"warmup_ratio\": 0.1,\n",
        "        # Memory optimization\n",
        "        \"gradient_checkpointing\": True,\n",
        "        \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
        "        # Evaluation and saving\n",
        "        \"eval_strategy\": \"epoch\",\n",
        "        \"save_strategy\": \"epoch\",\n",
        "        \"save_total_limit\": 2,\n",
        "        \"load_best_model_at_end\": True,\n",
        "        \"metric_for_best_model\": \"eval_loss\",\n",
        "        \"greater_is_better\": False,\n",
        "        # Logging and output\n",
        "        \"logging_steps\": 5,\n",
        "        \"report_to\": \"tensorboard\",\n",
        "        \"logging_dir\": \"logs/runs\",\n",
        "        \"overwrite_output_dir\": True,\n",
        "        # Model sharing\n",
        "        \"push_to_hub\": False,\n",
        "        \"hub_private_repo\": False,\n",
        "    }\n",
        "    fp16 = False\n",
        "    bf16 = True"
      ],
      "metadata": {
        "id": "h_bQRd22d-CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell creates an instance of the `Config` class and sets up the computation data type and device.\n",
        "\n",
        "- `config = Config()`: Creates an object `config` holding all the settings defined in the `Config` class.\n",
        "- `compute_dtype = torch.bfloat16`: Sets the desired data type for model computations based on the configuration (`bf16=True`). `bfloat16` offers memory savings and faster computation on compatible hardware compared to `float32`.\n",
        "- `device = \"cuda\"`: Explicitly sets the target device for computation to \"cuda\" (GPU). Assumes a CUDA-enabled GPU is available."
      ],
      "metadata": {
        "id": "csj0jRiafPxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "compute_dtype = torch.bfloat16\n",
        "device = \"cuda\""
      ],
      "metadata": {
        "id": "jHxefeijd-CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell loads the tokenizer associated with the specified base model and configures it with the custom special tokens.\n",
        "\n",
        "- `AutoTokenizer.from_pretrained(config.model_name, ...)`: Loads the tokenizer corresponding to the `google/gemma-3-1b-it` model.\n",
        "- `pad_token=ChatmlSpecialTokens.pad_token.value`: Explicitly sets the padding token to `<pad>` as defined in the `ChatmlSpecialTokens` enum. This ensures consistency, especially important if the base model doesn't have a pad token or uses a different one.\n",
        "- `additional_special_tokens=ChatmlSpecialTokens.list()`: Adds all the custom tokens defined in `ChatmlSpecialTokens` (like `<tools>`, `<think>`, etc.) to the tokenizer's vocabulary. This is crucial so the tokenizer recognizes these tokens and assigns them unique IDs."
      ],
      "metadata": {
        "id": "LYMNzAj5fYLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "        config.model_name,\n",
        "        pad_token=ChatmlSpecialTokens.pad_token.value,\n",
        "        additional_special_tokens=ChatmlSpecialTokens.list(),\n",
        "    )"
      ],
      "metadata": {
        "id": "4h3xIkqJd-CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the chat template used by the tokenizer to format conversational data.\n",
        "\n",
        "- **Purpose:** A chat template dictates how a list of messages (each with a 'role' like 'user', 'assistant' and 'content') is converted into a single string that the model can process. This formatting includes adding special control tokens (like start/end of turn markers, EOS tokens) that the model was trained to recognize.\n",
        "- **Template Structure:**\n",
        "   - `{{ bos_token }}`: Adds the beginning-of-sequence token at the start.\n",
        "   - `{% for message in messages %}`: Iterates through the messages in the conversation.\n",
        "   - `{% if message['role'] != 'system' %}`: This specific template skips messages with the 'system' role.\n",
        "   - `{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}`: For non-system messages, it formats them using Gemma's instruction-following format:\n",
        "     - `<start_of_turn>`: Marks the beginning of a turn.\n",
        "     - `message['role']`: Includes the role (e.g., 'user', 'assistant', 'tool').\n",
        "     - `\\n`: Newline.\n",
        "     - `message['content'] | trim`: The actual message content, with leading/trailing whitespace removed.\n",
        "     - `<end_of_turn>`: Marks the end of the turn.\n",
        "     - `<eos>`: Adds an end-of-sequence token after each turn.\n",
        "     - `\\n`: Newline.\n",
        "   - `{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}`: If requested during generation, adds the prompt for the model's turn.\n",
        "- **Import:** Using the correct chat template matching the model's fine-tuning (here, Gemma's instruction tuning format) is critical for effective instruction following and conversational ability. The custom function calling tokens will appear within the `message['content']`."
      ],
      "metadata": {
        "id": "ye8HUB2Gfxfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.chat_template = (\n",
        "    \"{{ bos_token }}{% for message in messages %}{% if message['role'] != 'system' %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
        ")"
      ],
      "metadata": {
        "id": "wW3HzsZpfvDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell loads the pre-trained causal language model specified in the configuration.\n",
        "\n",
        " - `AutoModelForCausalLM.from_pretrained(config.model_name, ...)`: Loads the `google/gemma-3-1b-it` model weights and architecture.\n",
        " - `torch_dtype=compute_dtype`: Loads the model weights using the specified data type (`torch.bfloat16`). This reduces memory footprint and potentially speeds up computation on compatible hardware.\n",
        " - `attn_implementation=\"eager\"`: Specifies the attention mechanism implementation. \"eager\" refers to the default PyTorch implementation. This might be explicitly set for compatibility or if optimized implementations like \"flash_attention_2\" are unavailable or cause issues.\n",
        " - `low_cpu_mem_usage=True`: Attempts to reduce peak CPU RAM usage during model loading by loading the state dictionary shard by shard. Useful for very large models.\n",
        " - `device_map=\"cpu\"`: Initially loads the model onto the CPU RAM. This is a strategy to avoid potential out-of-memory errors on the GPU if the full model doesn't fit alongside other requirements during the loading phase itself. The model will be moved to the GPU later.\n",
        "\n",
        "In addition:\n",
        "\n",
        " - `model.resize_token_embeddings(len(tokenizer))`: Resizes the model's token embedding layer to match the tokenizer's vocabulary size. This is **essential** because new special tokens were added to the tokenizer in step 6. This ensures the model has corresponding embedding vectors for these new tokens, which can be trained.\n",
        " - `model = model.to(device)`: Moves the entire model (including the potentially resized embedding layer) from the CPU (where it was initially loaded) to the target computation device (`cuda` / GPU). This is necessary for GPU-accelerated training."
      ],
      "metadata": {
        "id": "LfqX9OTDf_Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.model_name,\n",
        "    torch_dtype=compute_dtype,\n",
        "    attn_implementation=\"eager\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"cpu\",\n",
        ")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZjjNiHjfw1o",
        "outputId": "f14c1c74-4c80-4271-c090-329275a8945f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines a function `preprocess_and_filter` to prepare individual dataset samples for the `SFTTrainer`.\n",
        "\n",
        " - **Purpose:** To format the raw message data using the chat template and filter out sequences that are too long.\n",
        " - **Steps:**\n",
        "   1. Takes a `sample` (a dictionary expected to contain a \"messages\" key).\n",
        "   2. Extracts the `messages` list.\n",
        "   3. Uses `tokenizer.apply_chat_template(messages, tokenize=False)` to convert the list of message dictionaries into a single formatted string according to the template defined in step 7.\n",
        "   4. Encodes the resulting `text` into token IDs using `tokenizer.encode(text, truncation=False)`. Crucially, `truncation=False` is used here to get the *full* token length.\n",
        "   5. Checks if the number of tokens (`len(tokens)`) is less than or equal to the configured `max_seq_length` from `config.training_arguments`.\n",
        "   6. **If within limit:** Returns a dictionary `{\"text\": text}` containing the formatted string. `SFTTrainer` typically expects input data in a column named \"text\".\n",
        "   7. **If too long:** Returns `None`. This signals to the subsequent `.filter()` operation that this sample should be discarded.\n",
        " - **Rationale:** Ensures that all sequences used for training fit within the model's context window (`max_seq_length`), preventing errors and avoiding unwanted truncation by the trainer later. Filtering upfront is generally cleaner."
      ],
      "metadata": {
        "id": "SF-wbJ2fgjJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_filter(sample):\n",
        "  \"\"\"Preprocesses and filters a sample based on token length\"\"\"\n",
        "  messages = sample[\"messages\"]\n",
        "  text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "  tokens = tokenizer.encode(text, truncation=False)\n",
        "\n",
        "  if len(tokens) <= config.training_arguments[\"max_seq_length\"]:\n",
        "    return {\"text\": text}\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "Lsv2hUagd-CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = (load_dataset(config.dataset_name, split=\"train\")\n",
        "        .rename_column(\"conversations\", \"messages\")\n",
        "        .map(preprocess_and_filter, remove_columns=\"messages\")\n",
        "        .filter(lambda x: x is not None, keep_in_memory=False)\n",
        "    )"
      ],
      "metadata": {
        "id": "E31WumStd-CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell splits the processed dataset into training, validation and testing subsets.\n",
        "\n",
        "- `dataset_splits = data.train_test_split(test_size=0.2, shuffle=True, seed=0)`: Takes the 'train' split of the loaded and processed `data` (assuming the original dataset had a 'train' split) and splits it further. 80% of the data is kept for training (becomes the new 'train' split), and 20% is held out for evaluation (becomes the 'test' split). The `shuffle` and `seed` options help make the choice random and deterministic.\n",
        "- **Purpose:** Creating separate train, validation and test sets is crucial for evaluating the model's generalization performance. The model learns from the 'train' set, and its performance on the unseen 'validation' set indicates how well it might perform on new, similar data. A 80/20 split is a common practice. An holdout test set (`dataset_test`) is used then for the final evaluation."
      ],
      "metadata": {
        "id": "DRwTkqFv4lU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = data.train_test_split(test_size=0.2, shuffle=True, seed=0)\n",
        "dataset_test = load_dataset(config.dataset_name, split=\"test\")"
      ],
      "metadata": {
        "id": "jnoMXRGLd-CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now work on a few functions that are important for evaluating the Gemma 3 baseline for function calling (just by means of a prompt) and after fine tuning.\n",
        "\n",
        "The first function processes a batch of conversations, generates model responses for each, and returns the decoded text of these responses. This is essential just to interact with Gemma 3 in an easy and fast way.\n",
        "\n",
        "- `def generate_from_model_batch(batch_conversations, model, tokenizer):`\n",
        "    - **Purpose:** To generate text completions for a batch of input conversations.\n",
        "    - **Arguments:**\n",
        "        - `batch_conversations`: A list of conversation objects. Each conversation is typically a list of dictionaries, where each dictionary has 'role' (e.g., 'user', 'assistant') and 'content' (the message text) keys.\n",
        "        - `model`: The pre-trained language model (a Hugging Face Transformer model, in our example Gemma 3-1b-it) that will be used for generation.\n",
        "        - `tokenizer`: The tokenizer corresponding to the `model`, used for converting text to token IDs and vice-versa.\n",
        "\n",
        "- `prompts = [tokenizer.apply_chat_template(conv, tokenize=False) for conv in batch_conversations]`\n",
        "    - **Purpose:** Converts each structured conversation in the batch into a single formatted string prompt that the model can understand.\n",
        "    - `tokenizer.apply_chat_template`: This method takes a conversation (list of turns) and applies the model's specific chat template (e.g., adding special tokens for user/assistant roles, system prompts) to create a flat string.\n",
        "    - `tokenize=False`: Ensures the output is a string, not token IDs at this stage.\n",
        "\n",
        "- `inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048, add_special_tokens=False).to(device)`\n",
        "    - **Purpose:** Tokenizes the formatted string prompts and prepares them as input tensors for the model.\n",
        "    - `tokenizer(prompts, ...)`: Converts the list of prompt strings into token IDs.\n",
        "        - `return_tensors=\"pt\"`: Returns PyTorch tensors.\n",
        "        - `padding=True`: Pads shorter sequences in the batch with padding tokens to match the length of the longest sequence (or `max_length`).\n",
        "        - `truncation=True`: Truncates sequences that are longer than `max_length`.\n",
        "        - `max_length=2048`: Sets the maximum number of tokens for the input sequence (prompt).\n",
        "        - `add_special_tokens=False`: Assumes that `apply_chat_template` has already added any necessary special tokens (like BOS/EOS for the entire prompt, or role-specific tokens). This prevents the tokenizer from adding its default special tokens again.\n",
        "    - `.to(device)`: Moves the input tensors to the specified computation `device` (e.g., 'cuda' for GPU or 'cpu'). The `device` variable is assumed to be defined elsewhere in the scope.\n",
        "\n",
        "- `outputs = model.generate(...)`\n",
        "    - **Purpose:** Generates token sequences (responses) from the model based on the input prompts and specified generation parameters.\n",
        "    - `**inputs`: Unpacks the dictionary returned by the tokenizer (containing `input_ids`, `attention_mask`, etc.) as keyword arguments to the `model.generate` method.\n",
        "    - `max_new_tokens=256`: The model will generate at most 256 new tokens after the input prompt.\n",
        "    - `do_sample=True`: Enables sampling-based generation. If `False`, greedy decoding would be used.\n",
        "    - `top_p=0.95`: (top p sampling or nucleus sampling) At each step, considers the smallest set of tokens whose cumulative probability is at least 0.95. The model then samples from this set.\n",
        "    - `temperature=0.01`: Controls the randomness of the sampling. A very low temperature (like 0.01) makes the output more deterministic and less random, favoring higher probability tokens.\n",
        "    - `repetition_penalty=1.0`: A value of 1.0 means no penalty for repetition. Values > 1 penalize repeated tokens/phrases.\n",
        "    - `eos_token_id=tokenizer.eos_token_id`: Specifies the token ID that signifies the end of a sequence, so the model knows when to stop generating.\n",
        "\n",
        "- `prompt_lengths = [len(tokenizer(prompt)[\"input_ids\"]) for prompt in prompts]`\n",
        "    - **Purpose:** Calculates the length (in number of tokens) of each original input prompt. This is crucial for separating the generated text from the input prompt in the next step.\n",
        "    - It re-tokenizes each prompt string (as done before creating `inputs`) to get its length.\n",
        "\n",
        "- `generated_decoded = []`\n",
        "- `for i, output in enumerate(outputs):`\n",
        "  - `generated = tokenizer.decode(output[prompt_lengths[i]:], skip_special_tokens=False)`\n",
        "    - **Purpose:** Decodes the generated part of each output sequence back into human-readable text.\n",
        "    - `output`: Each `output` from `model.generate` contains the token IDs for the *entire* sequence (original prompt + generated tokens).\n",
        "    - `output[prompt_lengths[i]:]`: Slices the `output` tensor to get only the token IDs corresponding to the *newly generated* tokens, by skipping the tokens of the original prompt.\n",
        "    - `tokenizer.decode(...)`: Converts these generated token IDs back into a string.\n",
        "    - `skip_special_tokens=False`: Special tokens (like `<|endoftext|>`, padding tokens if any were part of the generation before EOS) within the generated portion will *not* be removed from the decoded string.\n",
        "  - `generated_decoded.append(generated.strip())`\n",
        "    - `strip()`: Removes any leading or trailing whitespace from the decoded generated string.\n",
        "    - The cleaned-up generated string is added to the `generated_decoded` list.\n",
        "\n",
        "- `return generated_decoded`\n",
        "    - **Purpose:** Returns a list of strings, where each string is the model-generated response corresponding to an input conversation from the batch."
      ],
      "metadata": {
        "id": "OrkK5JVV1ioo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_from_model_batch(batch_conversations, model, tokenizer):\n",
        "  prompts = [tokenizer.apply_chat_template(conv, tokenize=False) for conv in batch_conversations]\n",
        "\n",
        "  inputs = tokenizer(prompts,\n",
        "                     return_tensors=\"pt\",\n",
        "                     padding=True,\n",
        "                     truncation=True,\n",
        "                     max_length=2048,\n",
        "                     add_special_tokens=False).to(device)\n",
        "\n",
        "  outputs = model.generate(\n",
        "      **inputs,\n",
        "      max_new_tokens=256,\n",
        "      do_sample=True,\n",
        "      top_p=0.95,\n",
        "      temperature=0.01,\n",
        "      repetition_penalty=1.0,\n",
        "      eos_token_id=tokenizer.eos_token_id,\n",
        "  )\n",
        "\n",
        "  # Get lengths of prompts\n",
        "  prompt_lengths = [len(tokenizer(prompt)[\"input_ids\"]) for prompt in prompts]\n",
        "\n",
        "  # Decode outputs, excluding the prompt portion\n",
        "  generated_decoded = []\n",
        "  for i, output in enumerate(outputs):\n",
        "      generated = tokenizer.decode(output[prompt_lengths[i]:], skip_special_tokens=False)\n",
        "      generated_decoded.append(generated.strip())\n",
        "\n",
        "  return generated_decoded"
      ],
      "metadata": {
        "id": "FTSsG-z9qyS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following functions provide utilities for comparing two lists, focusing on different aspects of their similarity. The first function will help you evaluate the generated contents not function calling by comparing the bag of words of the generated answers and the ground truth (which we know are useful answers to the user).\n",
        "\n",
        "- `def compute_matching_percentage(list1, list2)`: Computes the percentage of matching elements between two lists. It first checks if either list is empty, returning `0.0` if so. Then, it uses `collections.Counter` to get frequency counts of elements in both `list1` and `list2`. The number of matches is calculated by summing the minimum count of each common element found in both lists. Finally, this total number of matches is divided by the length of `list2` to determine the matching percentage.\n",
        "- **Purpose:** This function provides a measure of how much of `list2` is \"covered\" or represented by `list1`, taking into account the frequency of duplicate items. It is useful for comparing multisets where the order of elements is not important, but their presence and frequency are significant (e.g., comparing sets of keywords or item features, where `list2` might be a reference set).\n",
        "\n",
        "As for the second function, it evaluates if the generated function calling matches the expected call from ground truth. It will look for the longest exact match and use that for scoring the result:\n",
        "\n",
        "- `def find_longest_common_sequence_length(list1, list2)`: Finds the length of the longest common *contiguous* sequence between two lists. If either input list is empty, it returns `0`. The function employs a dynamic programming approach, using `prev_row` and `current_row` to store lengths of common sequences ending at the current positions, which optimizes space. It iterates through `list1` and `list2`; if elements at the current positions match, the length of the common sequence (`current_row[j]`) is incremented based on the previous diagonal value (`prev_row[j-1] + 1`). If they don't match, the contiguous sequence is broken, and `current_row[j]` is set to `0`. The `max_length` variable keeps track of the longest sequence found.\n",
        "- **Purpose:** This function is useful for determining the extent of exact, ordered similarity between two sequences. Unlike `compute_matching_percentage` which looks at overall element overlap, this function focuses specifically on identical, uninterrupted blocks of elements. This is applicable in scenarios such as comparing sequences of events, detecting plagiarism by comparing sequences of words or characters, or analyzing genetic sequences for significant shared contiguous segments."
      ],
      "metadata": {
        "id": "09derDir1j8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_matching_percentage(list1, list2):\n",
        "    \"\"\"Computes the percentage of matching elements between two lists.\"\"\"\n",
        "    if not list1 or not list2:\n",
        "        return 0.0\n",
        "    count1, count2 = Counter(list1), Counter(list2)\n",
        "    matches = sum(min(count1[code], count2[code]) for code in count1 if code in count2)\n",
        "    return matches / len(list2)\n",
        "\n",
        "\n",
        "def find_longest_common_sequence_length(list1, list2):\n",
        "    \"\"\"Finds the length of the longest common contiguous sequence between two lists.\"\"\"\n",
        "    if not list1 or not list2:\n",
        "        return 0\n",
        "    m, n = len(list1), len(list2)\n",
        "    prev_row = [0] * (n + 1)\n",
        "    current_row = [0] * (n + 1)\n",
        "    max_length = 0\n",
        "    for i in range(1, m + 1):\n",
        "        prev_row, current_row = current_row, prev_row\n",
        "        for j in range(1, n + 1):\n",
        "            if list1[i - 1] == list2[j - 1]:\n",
        "                current_row[j] = prev_row[j - 1] + 1\n",
        "                max_length = max(max_length, current_row[j])\n",
        "            else:\n",
        "                current_row[j] = 0\n",
        "    return max_length"
      ],
      "metadata": {
        "id": "4V3kcka2EEdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function evaluates a model's ability to correctly generate tool calls (function calls) and provide helpful responses when no tool call is expected, by comparing its outputs against a dataset of ground truth conversations.\n",
        "\n",
        "- `def evaluate_function_calling(dataset, model, tokenizer, batch_size=8):`\n",
        "    - **Purpose:** To assess the model's performance on tasks involving potential function/tool calls and general conversational responses.\n",
        "    - **Arguments:**\n",
        "        - `dataset`: A list of conversation examples. Each example is expected to be a dictionary with a \"conversations\" key, which holds a list of dialogue turns (each turn being a dictionary with \"role\" and \"content\").\n",
        "        - `model`: The pre-trained language model to be evaluated.\n",
        "        - `tokenizer`: The tokenizer corresponding to the `model`.\n",
        "        - `batch_size=8`: The number of conversation queries to process in a single batch during generation.\n",
        "\n",
        "- `test_examples = len(dataset)`\n",
        "    - **Purpose:** Stores the total number of conversation examples in the provided `dataset`.\n",
        "\n",
        "- `tooling = []`, `being_useful = []`, `queries = []`, `answers = []`\n",
        "    - **Purpose:** Initializes empty lists to store evaluation metrics and intermediate data.\n",
        "        - `tooling`: Will store match scores for responses where a tool call was expected.\n",
        "        - `being_useful`: Will store match scores for responses where a general helpful answer was expected (no tool call).\n",
        "        - `queries`: Will store the input prompts (conversation history up to the point where the model should respond).\n",
        "        - `answers`: Will store the ground truth (expected) model responses.\n",
        "\n",
        "- `for i in range(test_examples): ...`\n",
        "    - **Purpose:** Loop through each conversation example in the `dataset`.\n",
        "\n",
        "- `conversations = []`\n",
        "    - **Purpose:** For each example, initializes an empty list to accumulate the turns of the current conversation history that will form the prompt.\n",
        "\n",
        "- `for item in dataset[i][\"conversations\"]:`\n",
        "    - **Purpose:** Loop through each turn within the current conversation example.\n",
        "\n",
        "- `if item[\"role\"] != \"model\": conversations.append(item)`\n",
        "    - **Purpose:** If the current turn is not from the \"model\" (e.g., \"user\", \"system\"), it's part of the input history. Append it to the `conversations` list that forms the prompt.\n",
        "\n",
        "- `if item[\"role\"] == \"model\":`\n",
        "    - **Purpose:** When a \"model\" turn is encountered, it means we have a complete prompt (the `conversations` accumulated so far) and a ground truth answer.\n",
        "    - `queries.append(conversations[:])`: Appends a *copy* of the current `conversations` (the prompt) to the `queries` list.\n",
        "    - `answers.append(item[\"content\"])`: Appends the actual content of the model's turn (the ground truth response) to the `answers` list.\n",
        "    - `conversations.append(item)`: Appends the current model's turn to `conversations`. This is important so that if the conversation continues with more user/model turns, this model response becomes part of the history for subsequent prompts within the same example.\n",
        "\n",
        "- `batches = [queries[i:i + batch_size] for i in range(0, len(queries), batch_size)]`\n",
        "    - **Purpose:** Groups the collected `queries` into smaller `batches` of the specified `batch_size` for efficient processing by the model.\n",
        "\n",
        "- `generated = []`\n",
        "    - **Purpose:** Initializes an empty list to store the responses generated by the model.\n",
        "\n",
        "- `for batch in tqdm(batches): generated.extend(generate_from_model_batch(batch, model, tokenizer))`\n",
        "    - **Purpose:** Iterates through each batch of `queries` (using `tqdm` for a progress bar) and generates model responses.\n",
        "    - `generate_from_model_batch(batch, model, tokenizer)`: Calls a separate function (presumably defined elsewhere, as in your previous example) to get model generations for the current `batch` of prompts.\n",
        "    - `.extend()`: Adds all generated responses from the current batch to the main `generated` list.\n",
        "\n",
        "- `for ground_truth, generated_response in zip(answers, generated):`\n",
        "    - **Purpose:** Iterates simultaneously through the list of `answers` (ground truth) and the list of `generated` responses from the model. `zip` pairs corresponding items.\n",
        "\n",
        "- `ground_truth_tokens = tokenizer(ground_truth)[\"input_ids\"]`\n",
        "- `generated_tokens = tokenizer(generated_response)[\"input_ids\"]`\n",
        "    - **Purpose:** Tokenizes both the ground truth string and the model-generated string into sequences of token IDs. This is done to compare them at a token level.\n",
        "\n",
        "- `if \"<tool_call>\" in ground_truth:`\n",
        "    - **Purpose:** Checks if the ground truth response was intended to be a tool call (signified by the presence of the `\"<tool_call>\"` string).\n",
        "    - `seq = find_longest_common_sequence_length(ground_truth_tokens, generated_tokens)`: Calls a helper function `find_longest_common_sequence_length` (assumed to be defined elsewhere) to find the length of the longest common subsequence between the token IDs of the ground truth and the generated response.\n",
        "    - `matches = seq / len(ground_truth_tokens)`: Calculates a match score as the ratio of the longest common subsequence length to the total length of the ground truth tokens. This gives a measure of how much of the expected tool call was correctly generated.\n",
        "    - `tooling.append(matches)`: Appends this match score to the `tooling` list.\n",
        "\n",
        "- `else:`\n",
        "    - **Purpose:** If the ground truth response was *not* a tool call, it's evaluated as a general helpful exchange.\n",
        "    - `matches = compute_matching_percentage(ground_truth_tokens, generated_tokens)`: Calls another helper function `compute_matching_percentage` (assumed to be defined elsewhere) to calculate a match score between the ground truth and generated tokens. This could be similar to LCS or another metric like ROUGE-L, or a custom token overlap.\n",
        "    - `being_useful.append(matches)`: Appends this match score to the `being_useful` list.\n",
        "\n",
        "- `print(f\"\\nAccuracy in function calling: {np.mean(tooling):0.5f}\")`\n",
        "- `print(f\"Match in helpful exchange: {np.mean(being_useful):0.5f}\")`\n",
        "    - **Purpose:** Calculates and prints the final evaluation metrics.\n",
        "    - `np.mean(tooling)`: Computes the average of all match scores for tool call responses.\n",
        "    - `np.mean(being_useful)`: Computes the average of all match scores for non-tool call (helpful) responses.\n",
        "    - `:0.5f`: Formats the output to display as a float with 5 decimal places."
      ],
      "metadata": {
        "id": "CpgFHXiU1k0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_function_calling(dataset, model, tokenizer, batch_size=8):\n",
        "    test_examples = len(dataset)\n",
        "    tooling = []\n",
        "    being_useful = []\n",
        "    queries =  []\n",
        "    answers = []\n",
        "\n",
        "    for i in range(test_examples):\n",
        "      conversations = []\n",
        "      for item in dataset[i][\"conversations\"]:\n",
        "          if item[\"role\"] != \"model\":\n",
        "              conversations.append(item)\n",
        "          if item[\"role\"] == \"model\":\n",
        "              queries.append(conversations[:])\n",
        "              answers.append(item[\"content\"])\n",
        "              conversations.append(item)\n",
        "\n",
        "    batches = [queries[i:i + batch_size] for i in range(0, len(queries), batch_size)]\n",
        "    generated = []\n",
        "    for batch in tqdm(batches):\n",
        "        generated.extend(generate_from_model_batch(batch, model, tokenizer))\n",
        "\n",
        "    for ground_truth, generated in zip(answers, generated):\n",
        "        ground_truth_tokens = tokenizer(ground_truth)[\"input_ids\"]\n",
        "        generated_tokens = tokenizer(generated)[\"input_ids\"]\n",
        "\n",
        "        # Evaluate function calling accuracy if tool call is present\n",
        "        if \"<tool_call>\" in ground_truth:\n",
        "            seq = find_longest_common_sequence_length(\n",
        "                ground_truth_tokens, generated_tokens\n",
        "            )\n",
        "            matches = seq / len(ground_truth_tokens)\n",
        "            tooling.append(matches)\n",
        "        else:\n",
        "            matches = compute_matching_percentage(\n",
        "                ground_truth_tokens, generated_tokens\n",
        "            )\n",
        "            being_useful.append(matches)\n",
        "\n",
        "    print(f\"\\nAccuracy in function calling: {np.mean(tooling):0.5f}\")\n",
        "    print(f\"Match in helpful exchange: {np.mean(being_useful):0.5f}\")"
      ],
      "metadata": {
        "id": "hs7PU1aWqy7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_function_calling(dataset_test.select(range(300)),\n",
        "                          model,\n",
        "                          tokenizer,\n",
        "                          batch_size=24)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyWzUG1k-U4Y",
        "outputId": "2ca345bd-68ba-4fe6-cfef-333dfb606910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46/46 [14:04<00:00, 18.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy in function calling: 0.35304\n",
            "Match in helpful exchange: 0.06495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell sets up the configuration for Parameter-Efficient Fine-Tuning (PEFT) using the LoRA technique.\n",
        "- `LoraConfig(...)`: Creates an instance of the LoRA configuration class.\n",
        "- `**config.lora_arguments`: Unpacks the dictionary of LoRA-specific hyperparameters (`r`, `lora_alpha`, `lora_dropout`, `target_modules`) defined earlier in the main `Config` class.\n",
        "- `task_type=TaskType.CAUSAL_LM`: Explicitly specifies that the PEFT technique (LoRA) is being applied to a Causal Language Model. This helps `peft` configure the model adaptation correctly for generation tasks.\n",
        "- **Purpose:** This `peft_config` object contains all the necessary information for the `SFTTrainer` to modify the base model by injecting LoRA adapters according to the specified parameters."
      ],
      "metadata": {
        "id": "wsU8VLXJ4qLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "        **config.lora_arguments,\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "    )"
      ],
      "metadata": {
        "id": "nok_J5xnd-CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell initializes the configuration object specifically required by the `SFTTrainer`.\n",
        "\n",
        " - `SFTConfig(...)`: Creates an instance of `SFTConfig`, which is a subclass of `transformers.TrainingArguments` tailored for the `SFTTrainer`.\n",
        " - `**config.training_arguments`: Unpacks the dictionary of general training hyperparameters (learning rate, batch size, epochs, optimization settings, logging, saving strategies, etc.) defined in the main `Config` class.\n",
        " - `output_dir=config.output_dir`: Explicitly sets the output directory where checkpoints and logs will be saved.\n",
        " - `fp16=config.fp16`, `bf16=config.bf16`: Sets the mixed-precision training flags based on the main configuration.\n",
        " - **Purpose:** This `training_arguments` object gathers all settings related to the training loop itself (optimization, scheduling, evaluation, saving, logging, etc.) into the format expected by the `SFTTrainer`.\n",
        "\n",
        " In addition:\n",
        "- `model.config.use_cache = False`: Disables the Key/Value (KV) cache mechanism in the model's attention layers. The KV cache speeds up *inference* by reusing past computations, but it's not needed during *training* and consumes significant GPU memory. Disabling it frees up memory, which is often crucial, especially when using gradient checkpointing.\n",
        "- `model.config.pretraining_tp = 1`: Sets the `pretraining_tp` (tensor parallelism used during pre-training) value to 1. This setting can sometimes be necessary for compatibility when fine-tuning models that were originally pre-trained with tensor parallelism, especially if the fine-tuning setup doesn't use the same degree of parallelism. Setting it to 1 essentially tells the configuration not to expect weights sharded in a particular way due to pre-training parallelism.\n",
        "- **Purpose:** These settings optimize the model configuration for the training phase, primarily focusing on memory efficiency (`use_cache=False`) and potential compatibility (`pretraining_tp=1`)."
      ],
      "metadata": {
        "id": "L2EhP4H35aM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = SFTConfig(\n",
        "    **config.training_arguments,\n",
        "    output_dir=config.output_dir,\n",
        "    fp16=config.fp16,\n",
        "    bf16=config.bf16,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "9VM-RRw55CK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell creates the `SFTTrainer` instance, which will manage the fine-tuning process.\n",
        "\n",
        "- `SFTTrainer(...)`: Initializes the trainer class from the `trl` library.\n",
        "- **Arguments:**\n",
        "   - `model=model`: The language model to be fine-tuned. The `peft` library will automatically modify this model based on `peft_config` when training starts.\n",
        "   - `args=training_arguments`: The `SFTConfig` object containing all training hyperparameters and settings.\n",
        "   - `train_dataset=dataset[\"train\"]`: The dataset split to be used for training.\n",
        "   - `eval_dataset=dataset[\"test\"]`: The dataset split to be used for evaluation.\n",
        "   - `tokenizer=tokenizer`: The tokenizer to be used for processing data (though much preprocessing was done manually here, the trainer might use it for collation or other internal steps). `processing_class` seems like a typo and likely should be `tokenizer`. Assuming it means `tokenizer`.\n",
        "   - `peft_config=peft_config`: The `LoraConfig` object specifying how LoRA should be applied. Passing this instructs the trainer to use PEFT.\n",
        "- **Purpose:** The `SFTTrainer` object encapsulates the model, data, tokenizer, and all configurations needed to run the supervised fine-tuning loop, handle evaluation, checkpointing, and logging.\n",
        "\n",
        "Then the cell initiates the actual fine-tuning process.\n",
        "\n",
        " - `trainer.train()`: Calls the `train` method of the `SFTTrainer` instance.\n",
        " - **Action:** This starts the training loop. The trainer will:\n",
        "   - Apply the LoRA modifications to the model based on `peft_config`.\n",
        "   - Iterate through the `train_dataset` for the specified number of epochs/steps.\n",
        "   - Compute loss, perform backpropagation, and update the LoRA adapter weights (and any other trainable parameters like embeddings).\n",
        "   - Perform evaluation on the `eval_dataset` based on the `eval_strategy`.\n",
        "   - Save model checkpoints based on the `save_strategy`.\n",
        "   - Log metrics according to `logging_steps` and `report_to`.\n",
        "   - Finally, load the best checkpoint if `load_best_model_at_end=True`.\n"
      ],
      "metadata": {
        "id": "fCGlHD-Q5qpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=dataset_train[\"train\"],\n",
        "    eval_dataset=dataset_train[\"test\"],\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=peft_config,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "RS4QydkKd-CT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "5b9f20992e494d7391c1a40522a576e1",
            "a899ccc4b42544cb9799c7de3447a49d",
            "d5baf675bb964f5aabde7fcacf781dbf",
            "f1254e05f6804295a9b1060a7f93563a",
            "b9686804aabf45499bc5192898891e53",
            "32a2f18fdee04eeca5b21159f4b3db8b",
            "4d0403ea15a24a01b093bd5521d82fb1",
            "6eb6cbafcbf94e18a7773d77adaba032",
            "1b3b936d62874c1da9524a6418a8c1c8",
            "b7fec746059e400b8e87d780f4d9c343",
            "ea356142db9142eeb252a0dd4c27af29",
            "851f299f626641d8bf531c2aef03e2d5",
            "b4468a196ad146ec8ab04bd4d16a9aeb",
            "77db871e1da9495a8f5fa8a337b518da",
            "5725fdf8f5984c87b154906d2523ee57",
            "2668332d53ff4a8b969f7b3af6ded37b",
            "e87e94976c7f4fb888229309b329a48e",
            "56194529b22b4b82997965a95ab2dc45",
            "61c8bc85b48a40e38203a764cfedf14f",
            "62ec77a30f634e44afaef93cda622948",
            "b6bc5e917eeb4eec89f8397abe25d402",
            "e6b54e94e7a540c8b9d4bbddbc1864c7",
            "2e6889d7f60644e78354f26b0cdf5a84",
            "b3152186d95d4083acb80810fa9cb89e",
            "30f49243e19841dea19eaea5d5f5f03e",
            "6036cb433bda4198a27c5bdd37854362",
            "b24da55610ee4c899d70c54a674558f8",
            "18bcbd8ef2c34566b57bf4555753e9a7",
            "0174939342a943688d4b370090934567",
            "0385595659e54877b8ef19aff4c49866",
            "b253c9d6eef64d24b6830693fd734ed4",
            "af14a847f8a44d95b2714990934b7f73",
            "6c153ce08e5d4ab18b1f5c347a0bba6c",
            "e5dbdf7a0b0043a0a8dc0e6e48ad43d2",
            "045483cc63fb4144910024520b023611",
            "918ab83817ba4199a69336327df30427",
            "477a782639b44745807bb4e3a4cc2eed",
            "15282ae0a06f40b0974dfa04d830e596",
            "5e6c4f7d08b646c9ac6064d5b8e6f490",
            "3d7714cbd5274dda96022943c91d593d",
            "37047688be0244a8aa6fb722fbb19c5e",
            "223e32432d2e4903930b14f8e4296136",
            "ca9218b76dcc40e5acd35cd600e1d144",
            "05520385b7f541728b52c29f241452fe"
          ]
        },
        "outputId": "258337ca-5b0c-4c04-8629-ce959cf24bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:550: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.embed_tokens', 'lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/3326 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b9f20992e494d7391c1a40522a576e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Packing train dataset:   0%|          | 0/3326 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "851f299f626641d8bf531c2aef03e2d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing eval dataset:   0%|          | 0/832 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e6889d7f60644e78354f26b0cdf5a84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Packing eval dataset:   0%|          | 0/832 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5dbdf7a0b0043a0a8dc0e6e48ad43d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='279' max='279' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [279/279 11:40, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.308200</td>\n",
              "      <td>0.316551</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=279, training_loss=0.5127464780670767, metrics={'train_runtime': 703.0779, 'train_samples_per_second': 1.587, 'train_steps_per_second': 0.397, 'total_flos': 9852602303980800.0, 'train_loss': 0.5127464780670767})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell saves the results of the fine-tuning process locally.\n",
        "\n",
        " - `trainer.model.save_pretrained(\"LoRA_\" + config.output_dir, save_embedding_layers=True)`: Saves the trained PEFT adapter weights (the LoRA layers) to a directory named \"LoRA_gemma-3-1B-it-function_calling\". Because LoRA was used, this saves only the small adapter weights, not the entire base model. `save_embedding_layers=True` attempts to save the fine-tuned input/output embedding layers if they were targeted by LoRA or resized and made trainable; this behavior can vary across library versions.\n",
        " - `tokenizer.eos_token = \"<eos>\"`: Explicitly sets the `eos_token` attribute of the tokenizer object. This might be redundant if already configured but acts as a safeguard.\n",
        " - `tokenizer.save_pretrained(\"LoRA_\" + config.output_dir)`: Saves the tokenizer's configuration (including vocabulary, added special tokens like the ChatML ones, and the custom chat template) to the same directory as the LoRA adapters.\n",
        " - **Purpose:** Persists the fine-tuning results (adapter weights) and the corresponding tokenizer configuration needed to correctly load and use the fine-tuned model later for inference. Saving them together ensures compatibility."
      ],
      "metadata": {
        "id": "nioMHvp96Cel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving LoRA weights and tokenizer\n",
        "trainer.model.save_pretrained(\n",
        "    \"LoRA_\" + config.output_dir, save_embedding_layers=True\n",
        ")\n",
        "tokenizer.eos_token = \"<eos>\"\n",
        "tokenizer.save_pretrained(\"LoRA_\" + config.output_dir)"
      ],
      "metadata": {
        "id": "0WEsyn5ld-CT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b21e0f5-f8a1-49b3-c699-c6c1244a1806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('LoRA_gemma-3-1B-it-function_calling/tokenizer_config.json',\n",
              " 'LoRA_gemma-3-1B-it-function_calling/special_tokens_map.json',\n",
              " 'LoRA_gemma-3-1B-it-function_calling/tokenizer.model',\n",
              " 'LoRA_gemma-3-1B-it-function_calling/added_tokens.json',\n",
              " 'LoRA_gemma-3-1B-it-function_calling/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell handles authentication with the Hugging Face Hub, using secrets management within Google Colab.\n",
        "\n",
        " - `from huggingface_hub import login`: Imports the login function.\n",
        " - `from google.colab import userdata`: Imports the utility for accessing secrets stored in Colab.\n",
        " - `userdata.get('HF_TOKEN')`: Attempts to retrieve a secret named 'HF_TOKEN' (stored in Colab), which should contain a Hugging Face API token with write permissions.\n",
        " - `login(hf_token)`: If the token is found, this function authenticates the Colab environment with the Hugging Face Hub, allowing subsequent push operations.\n",
        " - **Purpose:** Securely authenticates the session to allow uploading the fine-tuned adapter and tokenizer to a user's repository on the Hugging Face Hub. Using secrets avoids hardcoding sensitive tokens in the notebook. If you want to use your own secrets on Colab, too, have a look at the icon bar on the left and click on the key icon. You will be shown an interface where you can add a secret by name and relative value, decide what secrets are accessible by the notebook and furthermore manage your secrets by copying, discarding or importing them from Google AI Studio (for instance Gemini API keys)."
      ],
      "metadata": {
        "id": "krHlf8oR6DFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if hf_token:\n",
        "    login(hf_token)\n",
        "    print(\"Successfully logged in!\")\n",
        "else:\n",
        "    print(\"Token not found. Check Secrets configuration.\")"
      ],
      "metadata": {
        "id": "4-JzB3gid-CT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce204ee4-add3-480e-b4f2-c483dc5b8d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully logged in!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell uploads the saved LoRA adapter weights and the tokenizer configuration to the specified repository on the Hugging Face Hub.\n",
        "\n",
        " - `username=\"lmassaron\"`: **Placeholder:** Replace my user name `\"lmassaron\"` with your actual Hugging Face Hub username.\n",
        " - `output_dir = \"gemma-3-1B-it-function_calling\"`: This is the choosen name for the repository on the Hub. You can opt for a different name.\n",
        " - `trainer.push_to_hub(f\"{username}/{output_dir}\")`: Uploads the contents of the local directory where the adapter was saved (by `trainer.model.save_pretrained`) to the specified Hub repository (`username/output_dir`). This includes the adapter weights (`adapter_model.safetensors`) and configuration (`adapter_config.json`).\n",
        " - `tokenizer.push_to_hub(f\"{username}/{output_dir}\", token=True)`: Uploads the tokenizer files (saved by `tokenizer.save_pretrained`) to the *same* Hub repository. `token=True` ensures the authentication token is used, though it might be implicit after `login`.\n",
        " - **Purpose:** Makes the fine-tuned LoRA adapter and its corresponding tokenizer publicly or privately accessible via the Hugging Face Hub, facilitating sharing, collaboration, and easy loading for inference elsewhere."
      ],
      "metadata": {
        "id": "B32GItfJ6TeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "username=\"lmassaron\"\n",
        "output_dir = \"gemma-3-1B-it-function_calling\"\n",
        "trainer.push_to_hub(f\"{username}/{output_dir}\")\n",
        "tokenizer.push_to_hub(f\"{username}/{output_dir}\", token=True)"
      ],
      "metadata": {
        "id": "uEbQIkEXd-CT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "bd977524-179d-47de-fb07-0a177af3e16f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/lmassaron/gemma-3-1B-it-function_calling/commit/b54d875e2bdc89539521509b74db8cdff270a626', commit_message='Upload tokenizer', commit_description='', oid='b54d875e2bdc89539521509b74db8cdff270a626', pr_url=None, repo_url=RepoUrl('https://huggingface.co/lmassaron/gemma-3-1B-it-function_calling', endpoint='https://huggingface.co', repo_type='model', repo_id='lmassaron/gemma-3-1B-it-function_calling'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_function_calling(dataset_test.select(range(300)),\n",
        "                          trainer.model,\n",
        "                          tokenizer,\n",
        "                          batch_size=24)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suwSACTO9rZz",
        "outputId": "588e7945-8f82-4cc8-f6d5-219c7a1e38b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46/46 [16:27<00:00, 21.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy in function calling: 0.93189\n",
            "Match in helpful exchange: 0.11841\n"
          ]
        }
      ]
    }
  ]
}