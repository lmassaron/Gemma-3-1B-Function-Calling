{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8a6f2dbe732d47d5ac3bd4e063e43cfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c5461fcb6e24b049c11f4555a7b02a3",
              "IPY_MODEL_49e271c17c05460db9a18ca14ab09672",
              "IPY_MODEL_a922078ae5184df0a14076e04ff2954e"
            ],
            "layout": "IPY_MODEL_fa2e51a8ced44e228077ef7ff0e1f3ce"
          }
        },
        "9c5461fcb6e24b049c11f4555a7b02a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd37c80b741a4f2f82955d861224be3b",
            "placeholder": "​",
            "style": "IPY_MODEL_319e82cbb49f4ccb856cc530691b34ce",
            "value": "Upload 2 LFS files: 100%"
          }
        },
        "49e271c17c05460db9a18ca14ab09672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2ec36dd3e29466ca531758b0966bd73",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fe3a14debf04839a327ab57646e582e",
            "value": 2
          }
        },
        "a922078ae5184df0a14076e04ff2954e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c1ae2a8206049fca3bb758b9efc3ab2",
            "placeholder": "​",
            "style": "IPY_MODEL_4553d3b338c04cd68fc84a36ef2876d3",
            "value": " 2/2 [01:08&lt;00:00, 68.10s/it]"
          }
        },
        "fa2e51a8ced44e228077ef7ff0e1f3ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd37c80b741a4f2f82955d861224be3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "319e82cbb49f4ccb856cc530691b34ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2ec36dd3e29466ca531758b0966bd73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fe3a14debf04839a327ab57646e582e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c1ae2a8206049fca3bb758b9efc3ab2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4553d3b338c04cd68fc84a36ef2876d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abba2dedec844721bdbf18264d1236e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8e0cd220a00480491b17ed9e3197a45",
              "IPY_MODEL_09b84bf712d84f758c73e2d05e191ef9",
              "IPY_MODEL_f1a158dccfc14811a23bde2923c2b759"
            ],
            "layout": "IPY_MODEL_91d53d3308a64c16ad7696a34ff1fdd2"
          }
        },
        "e8e0cd220a00480491b17ed9e3197a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e98c7e76947c4813b1ad29c67cee50a3",
            "placeholder": "​",
            "style": "IPY_MODEL_f5110673119a4999b08d9d40e6dff801",
            "value": "adapter_model.safetensors: 100%"
          }
        },
        "09b84bf712d84f758c73e2d05e191ef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3cc88ab86ae4bb08526688487b6665e",
            "max": 1293936232,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65c7b792b5904c328efe2138b0575a23",
            "value": 1293936232
          }
        },
        "f1a158dccfc14811a23bde2923c2b759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c3650281ad345d198988db43c46487c",
            "placeholder": "​",
            "style": "IPY_MODEL_4102c0a204b14572974c99b094931bd9",
            "value": " 1.29G/1.29G [01:07&lt;00:00, 24.1MB/s]"
          }
        },
        "91d53d3308a64c16ad7696a34ff1fdd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e98c7e76947c4813b1ad29c67cee50a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5110673119a4999b08d9d40e6dff801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3cc88ab86ae4bb08526688487b6665e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65c7b792b5904c328efe2138b0575a23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c3650281ad345d198988db43c46487c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4102c0a204b14572974c99b094931bd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6760ad37840f4e368c1d35b02e255930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32b09548287f4839aaf5dd33a06c43c9",
              "IPY_MODEL_efa0f0c420a148bbb7554b42c188a62f",
              "IPY_MODEL_7fe6c9c063c44830b8125b0e1f7aebbf"
            ],
            "layout": "IPY_MODEL_b675337d45234c3da23cc1910ff45d5e"
          }
        },
        "32b09548287f4839aaf5dd33a06c43c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce8fa61404e142fbb59c5e314cf987dc",
            "placeholder": "​",
            "style": "IPY_MODEL_3699082f904d4319a293745d17f1c5cb",
            "value": "training_args.bin: 100%"
          }
        },
        "efa0f0c420a148bbb7554b42c188a62f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e39455584b74ec59914fc9e08397875",
            "max": 5624,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db59bd0358f84740bb90cce9feda0ba8",
            "value": 5624
          }
        },
        "7fe6c9c063c44830b8125b0e1f7aebbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97a58047708a47b190584d1767049902",
            "placeholder": "​",
            "style": "IPY_MODEL_185f30d2dd3e4276942f356663a6db62",
            "value": " 5.62k/5.62k [00:00&lt;00:00, 18.8kB/s]"
          }
        },
        "b675337d45234c3da23cc1910ff45d5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce8fa61404e142fbb59c5e314cf987dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3699082f904d4319a293745d17f1c5cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e39455584b74ec59914fc9e08397875": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db59bd0358f84740bb90cce9feda0ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "97a58047708a47b190584d1767049902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "185f30d2dd3e4276942f356663a6db62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0f0466c2705494bae6fe10bcc70e9cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42d8aa5b21884e0ab1a43b47aaa65c5c",
              "IPY_MODEL_e73c78f0ae1d471b8e69dc52f27ca3e4",
              "IPY_MODEL_2b8b22fc5f1743f1a96ba55989460d88"
            ],
            "layout": "IPY_MODEL_bef2be5b4a6b474f8c37dcd8afd82873"
          }
        },
        "42d8aa5b21884e0ab1a43b47aaa65c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_067fef76009d42c6ad5f08a0c94af3dd",
            "placeholder": "​",
            "style": "IPY_MODEL_9313cf536a2a481090c326b88b0c6c88",
            "value": "README.md: 100%"
          }
        },
        "e73c78f0ae1d471b8e69dc52f27ca3e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5c96f69027d41d2843e930d829f856a",
            "max": 1501,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f977c73bedf94dbe914152512414c193",
            "value": 1501
          }
        },
        "2b8b22fc5f1743f1a96ba55989460d88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b183c01d1ec4f218b31a147023cb71a",
            "placeholder": "​",
            "style": "IPY_MODEL_d9caf98009764f7ab9215f4228359933",
            "value": " 1.50k/1.50k [00:00&lt;00:00, 140kB/s]"
          }
        },
        "bef2be5b4a6b474f8c37dcd8afd82873": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "067fef76009d42c6ad5f08a0c94af3dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9313cf536a2a481090c326b88b0c6c88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5c96f69027d41d2843e930d829f856a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f977c73bedf94dbe914152512414c193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b183c01d1ec4f218b31a147023cb71a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9caf98009764f7ab9215f4228359933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This cell installs the necessary Python libraries required for the fine-tuning process.\n",
        "\n",
        "- `transformers`: Provides access to pre-trained models (like Gemma), tokenizers, and training utilities from Hugging Face.\n",
        "- `accelerate`: Simplifies running PyTorch code on various hardware setups (CPU, single/multi-GPU, TPU) and handles mixed-precision training.\n",
        "- `datasets`: Used for efficiently loading, processing, and manipulating datasets, especially those hosted on the Hugging Face Hub.\n",
        "- `peft`: (Parameter-Efficient Fine-Tuning) Enables techniques like LoRA (Low-Rank Adaptation) to fine-tune large models efficiently by training only a small number of extra parameters.\n",
        "- `trl`: (Transformer Reinforcement Learning library) Provides tools for fine-tuning language models, including the `SFTTrainer` used here for Supervised Fine-Tuning."
      ],
      "metadata": {
        "id": "ppSo_A50d7Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U trl"
      ],
      "metadata": {
        "id": "C6pflsoAd-CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell imports the required modules and classes from the installed libraries and Python's standard library. In particular notice:\n",
        "\n",
        "- `os`: Used for interacting with the operating system (though not directly used in the visible snippet, often useful for path manipulation).\n",
        "- `enum.Enum`: Used to create enumeration types, here specifically for defining special tokens in a structured way.\n",
        "- `torch`: The core PyTorch library for tensor computations and neural network modules.\n",
        "- `transformers`: Imports `AutoModelForCausalLM` (to load the language model), `AutoTokenizer` (to load the tokenizer), and `set_seed` (for reproducibility, although not used here).\n",
        "- `datasets`: Imports `load_dataset` for fetching data from the Hugging Face Hub.\n",
        "- `trl`: Imports `SFTConfig` (configuration for supervised fine-tuning) and `SFTTrainer` (the class that handles the training process).\n",
        "- `peft`: Imports `LoraConfig` (configuration for LoRA) and `TaskType` (to specify the type of task for PEFT, e.g., Causal LM)."
      ],
      "metadata": {
        "id": "2AV3ibJueKx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
        "from datasets import load_dataset\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from peft import LoraConfig, TaskType"
      ],
      "metadata": {
        "id": "BepmFj8Pd-CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)"
      ],
      "metadata": {
        "id": "x408hJAk8IW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines an `Enum` class `ChatmlSpecialTokens` to manage custom special tokens related to function/tool calling within the ChatML format.\n",
        "\n",
        "- **Purpose:** Using an Enum provides a robust and readable way to handle these tokens consistently throughout the script, avoiding potential typos with raw strings.\n",
        "- **Tokens:**\n",
        "    - `<tools>`, `</tools>`: Delimit a section describing available tools.\n",
        "    - `<think>`, `</think>`: Delimit the model's internal thought process before acting.\n",
        "    - `<tool_call>`, `</tool_call>`: Delimit the model's request to call a specific tool.\n",
        "    - `<tool_response>`, `</tool_response>`: Delimit the response received after executing a tool.\n",
        "    - `<pad>`: Padding token used to make sequences in a batch the same length.\n",
        "    - `<eos>`: End-of-sequence token, often used to signal the end of a generated turn or document.\n",
        "- **`list()` method:** A class method is provided to easily retrieve all defined special token values as a list, useful for adding them to the tokenizer.\n",
        "- **Relevance:** These tokens are crucial for training the model to understand and generate text involving tool interactions, as expected by the chosen dataset (`hermes-function-calling-v1`)."
      ],
      "metadata": {
        "id": "uRs3NrQ6edeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatmlSpecialTokens(str, Enum):\n",
        "    \"\"\"Enum class defining special tokens used in the ChatML format\"\"\"\n",
        "\n",
        "    tools = \"<tools>\"\n",
        "    eotools = \"</tools>\"\n",
        "    think = \"<think>\"\n",
        "    eothink = \"</think>\"\n",
        "    tool_call = \"<tool_call>\"\n",
        "    eotool_call = \"</tool_call>\"\n",
        "    tool_response = \"<tool_response>\"\n",
        "    eotool_response = \"</tool_response>\"\n",
        "    pad_token = \"<pad>\"\n",
        "    eos_token = \"<eos>\"\n",
        "\n",
        "    @classmethod\n",
        "    def list(cls):\n",
        "        return [c.value for c in cls]"
      ],
      "metadata": {
        "id": "ORgc0JQjd-CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell centralizes all configuration parameters for the fine-tuning script within a `Config` class.\n",
        "\n",
        " - **Purpose:** Grouping settings makes the script organized, easier to read, and simpler to modify hyperparameters.\n",
        " - **Key Parameters & Rationale:**\n",
        "   - `model_name`: \"google/gemma-3-1b-it\" - Specifies the base pre-trained model to fine-tune. Gemma-3-1B-IT is an instruction-tuned version of Google's Gemma model.\n",
        "   - `dataset_name`: \"lmassaron/hermes-function-calling-v1\" - The dataset used for fine-tuning, containing examples of conversations involving function/tool calls.\n",
        "   - `output_dir`: \"gemma-3-1B-it-function_calling\" - The directory where trained model artifacts (LoRA adapters, checkpoints) will be saved.\n",
        "   - `lora_arguments`: Configuration for LoRA (Parameter-Efficient Fine-Tuning).\n",
        "     - `r=16`: Rank of the LoRA matrices. A higher rank allows for more expressiveness but increases the number of trainable parameters. 16 is a common value offering a good balance.\n",
        "     - `lora_alpha=64`: Scaling factor for LoRA. Often set as 2x or 4x the rank (`r`). It controls the magnitude of the adaptation applied by the LoRA weights. `64` provides a strong scaling relative to `r=16`.\n",
        "     - `lora_dropout=0.05`: Dropout rate applied to LoRA layers to prevent overfitting during fine-tuning. 0.05 is a relatively low dropout rate.\n",
        "     - `target_modules`: List of modules within the base model where LoRA adapters will be injected. Targeting attention query/key/value/output projections (`q_proj`, `k_proj`, `v_proj`, `o_proj`) and feed-forward layers (`gate_proj`, `up_proj`, `down_proj`) is standard practice. Including `embed_tokens` and `lm_head` allows fine-tuning of input embeddings and the final output layer, potentially beneficial when adding special tokens or adapting to specific output formats.\n",
        "   - `training_arguments`: Configuration for the `SFTTrainer` (via `SFTConfig`, which inherits from `transformers.TrainingArguments`).\n",
        "     - `num_train_epochs=1`: The model will iterate over the entire training dataset once. Often sufficient for fine-tuning, especially with large datasets or effective techniques like LoRA, to prevent overfitting.\n",
        "     - `per_device_train_batch_size=1`: Number of samples processed per GPU per forward/backward pass during training. Kept low (1) likely due to GPU memory constraints with a large `max_seq_length`.\n",
        "     - `gradient_accumulation_steps=4`: Number of steps to accumulate gradients before performing a weight update. Simulates a larger batch size (`1 * 4 = 4`) without increasing memory usage proportionally. Helps stabilize training.\n",
        "     - `max_seq_length=2048`: Maximum token length for sequences fed into the model. Sequences longer than this will be filtered out. This value impacts memory usage significantly. 2048 is a reasonable context window for many modern models like Gemma.\n",
        "     - `packing=True`: Enables packing multiple short sequences into a single sequence up to `max_seq_length`, separated by EOS tokens. Improves training efficiency by reducing the amount of padding needed.\n",
        "     - `optim=\"adamw_torch_fused\"`: Specifies the AdamW optimizer implementation. The `_fused` version often provides better performance on GPUs.\n",
        "     - `learning_rate=1e-4`: The initial learning rate for the optimizer. `1e-4` is a common starting point for LoRA fine-tuning.\n",
        "     - `weight_decay=0.1`: Applies L2 regularization to prevent overfitting.\n",
        "     - `max_grad_norm=1.0`: Clips gradients to a maximum norm of 1.0 to prevent exploding gradients during training.\n",
        "     - `lr_scheduler_type=\"cosine\"`: Uses a cosine annealing learning rate scheduler, which gradually decreases the LR, often leading to better convergence.\n",
        "     - `warmup_ratio=0.1`: 10% of the total training steps will be used for a linear learning rate warm-up phase, starting from 0 and increasing to the `learning_rate`. Helps stabilize training early on.\n",
        "     - `gradient_checkpointing=True`: Saves significant GPU memory by recomputing activations during the backward pass instead of storing them all. Essential for training large models on limited hardware, at the cost of slightly slower training speed. `use_reentrant=False` is often recommended with newer PyTorch versions.\n",
        "     - `eval_strategy=\"epoch\"`, `save_strategy=\"epoch\"`: Perform evaluation and save model checkpoints at the end of each epoch.\n",
        "     - `load_best_model_at_end=True`: After training finishes, the trainer will load the checkpoint corresponding to the best evaluation metric.\n",
        "     - `metric_for_best_model=\"eval_loss\"`: The metric used to determine the \"best\" model (lower evaluation loss is better).\n",
        "     - `logging_steps=5`: Log training metrics (like loss) every 5 steps.\n",
        "     - `report_to=\"tensorboard\"`: Specifies that logs should be formatted for TensorBoard.\n",
        "     - `push_to_hub=False`: Whether to automatically push the model to the Hugging Face Hub after training (set to `True` later for explicit push).\n",
        "   - `fp16=False`, `bf16=True`: Configures mixed-precision training. `bf16` (BFloat16) is preferred on modern GPUs (Ampere architecture and newer) as it offers a better balance between speed/memory savings and numerical stability compared to `fp16` (Float16) for training large models."
      ],
      "metadata": {
        "id": "do7Q-E3-e0kV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    model_name = \"google/gemma-3-1b-it\"\n",
        "    dataset_name = \"lmassaron/hermes-function-calling-v1\"\n",
        "    output_dir = \"gemma-3-1B-it-function_calling\"\n",
        "    lora_arguments = {\n",
        "        \"r\": 16,\n",
        "        \"lora_alpha\": 64,\n",
        "        \"lora_dropout\": 0.05,\n",
        "        \"target_modules\": [\n",
        "            \"embed_tokens\",\n",
        "            \"q_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"gate_proj\",\n",
        "            \"up_proj\",\n",
        "            \"down_proj\",\n",
        "            \"o_proj\",\n",
        "            \"lm_head\",\n",
        "        ],\n",
        "    }\n",
        "    training_arguments = {\n",
        "        # Basic training configuration\n",
        "        \"num_train_epochs\": 1,\n",
        "        \"max_steps\": -1,\n",
        "        \"per_device_train_batch_size\": 1,\n",
        "        \"per_device_eval_batch_size\": 1,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"max_seq_length\": 2048,\n",
        "        \"packing\": True,\n",
        "        # Optimization settings\n",
        "        \"optim\": \"adamw_torch_fused\",\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"weight_decay\": 0.1,\n",
        "        \"max_grad_norm\": 1.0,\n",
        "        \"lr_scheduler_type\": \"cosine\",\n",
        "        \"warmup_ratio\": 0.1,\n",
        "        # Memory optimization\n",
        "        \"gradient_checkpointing\": True,\n",
        "        \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
        "        # Evaluation and saving\n",
        "        \"eval_strategy\": \"epoch\",\n",
        "        \"save_strategy\": \"epoch\",\n",
        "        \"save_total_limit\": 2,\n",
        "        \"load_best_model_at_end\": True,\n",
        "        \"metric_for_best_model\": \"eval_loss\",\n",
        "        \"greater_is_better\": False,\n",
        "        # Logging and output\n",
        "        \"logging_steps\": 5,\n",
        "        \"report_to\": \"tensorboard\",\n",
        "        \"logging_dir\": \"logs/runs\",\n",
        "        \"overwrite_output_dir\": True,\n",
        "        # Model sharing\n",
        "        \"push_to_hub\": False,\n",
        "        \"hub_private_repo\": False,\n",
        "    }\n",
        "    fp16 = False\n",
        "    bf16 = True"
      ],
      "metadata": {
        "id": "h_bQRd22d-CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell creates an instance of the `Config` class and sets up the computation data type and device.\n",
        "\n",
        "- `config = Config()`: Creates an object `config` holding all the settings defined in the `Config` class.\n",
        "- `compute_dtype = torch.bfloat16`: Sets the desired data type for model computations based on the configuration (`bf16=True`). `bfloat16` offers memory savings and faster computation on compatible hardware compared to `float32`.\n",
        "- `device = \"cuda\"`: Explicitly sets the target device for computation to \"cuda\" (GPU). Assumes a CUDA-enabled GPU is available."
      ],
      "metadata": {
        "id": "csj0jRiafPxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "compute_dtype = torch.bfloat16\n",
        "device = \"cuda\""
      ],
      "metadata": {
        "id": "jHxefeijd-CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell loads the tokenizer associated with the specified base model and configures it with the custom special tokens.\n",
        "\n",
        "- `AutoTokenizer.from_pretrained(config.model_name, ...)`: Loads the tokenizer corresponding to the `google/gemma-3-1b-it` model.\n",
        "- `pad_token=ChatmlSpecialTokens.pad_token.value`: Explicitly sets the padding token to `<pad>` as defined in the `ChatmlSpecialTokens` enum. This ensures consistency, especially important if the base model doesn't have a pad token or uses a different one.\n",
        "- `additional_special_tokens=ChatmlSpecialTokens.list()`: Adds all the custom tokens defined in `ChatmlSpecialTokens` (like `<tools>`, `<think>`, etc.) to the tokenizer's vocabulary. This is crucial so the tokenizer recognizes these tokens and assigns them unique IDs."
      ],
      "metadata": {
        "id": "LYMNzAj5fYLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "        config.model_name,\n",
        "        pad_token=ChatmlSpecialTokens.pad_token.value,\n",
        "        additional_special_tokens=ChatmlSpecialTokens.list(),\n",
        "    )"
      ],
      "metadata": {
        "id": "4h3xIkqJd-CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the chat template used by the tokenizer to format conversational data.\n",
        "\n",
        "- **Purpose:** A chat template dictates how a list of messages (each with a 'role' like 'user', 'assistant' and 'content') is converted into a single string that the model can process. This formatting includes adding special control tokens (like start/end of turn markers, EOS tokens) that the model was trained to recognize.\n",
        "- **Template Structure:**\n",
        "   - `{{ bos_token }}`: Adds the beginning-of-sequence token at the start.\n",
        "   - `{% for message in messages %}`: Iterates through the messages in the conversation.\n",
        "   - `{% if message['role'] != 'system' %}`: This specific template skips messages with the 'system' role.\n",
        "   - `{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}`: For non-system messages, it formats them using Gemma's instruction-following format:\n",
        "     - `<start_of_turn>`: Marks the beginning of a turn.\n",
        "     - `message['role']`: Includes the role (e.g., 'user', 'assistant', 'tool').\n",
        "     - `\\n`: Newline.\n",
        "     - `message['content'] | trim`: The actual message content, with leading/trailing whitespace removed.\n",
        "     - `<end_of_turn>`: Marks the end of the turn.\n",
        "     - `<eos>`: Adds an end-of-sequence token after each turn.\n",
        "     - `\\n`: Newline.\n",
        "   - `{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}`: If requested during generation, adds the prompt for the model's turn.\n",
        "- **Importance:** Using the correct chat template matching the model's fine-tuning (here, Gemma's instruction tuning format) is critical for effective instruction following and conversational ability. The custom function calling tokens will appear within the `message['content']`."
      ],
      "metadata": {
        "id": "ye8HUB2Gfxfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.chat_template = (\n",
        "    \"{{ bos_token }}{% for message in messages %}{% if message['role'] != 'system' %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
        ")"
      ],
      "metadata": {
        "id": "wW3HzsZpfvDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell loads the pre-trained causal language model specified in the configuration.\n",
        "\n",
        " - `AutoModelForCausalLM.from_pretrained(config.model_name, ...)`: Loads the `google/gemma-3-1b-it` model weights and architecture.\n",
        " - `torch_dtype=compute_dtype`: Loads the model weights using the specified data type (`torch.bfloat16`). This reduces memory footprint and potentially speeds up computation on compatible hardware.\n",
        " - `attn_implementation=\"eager\"`: Specifies the attention mechanism implementation. \"eager\" refers to the default PyTorch implementation. This might be explicitly set for compatibility or if optimized implementations like \"flash_attention_2\" are unavailable or cause issues.\n",
        " - `low_cpu_mem_usage=True`: Attempts to reduce peak CPU RAM usage during model loading by loading the state dictionary shard by shard. Useful for very large models.\n",
        " - `device_map=\"cpu\"`: Initially loads the model onto the CPU RAM. This is a strategy to avoid potential out-of-memory errors on the GPU if the full model doesn't fit alongside other requirements during the loading phase itself. The model will be moved to the GPU later.\n",
        "\n",
        "In addition:\n",
        "\n",
        " - `model.resize_token_embeddings(len(tokenizer))`: Resizes the model's token embedding layer to match the tokenizer's vocabulary size. This is **essential** because new special tokens were added to the tokenizer in step 6. This ensures the model has corresponding embedding vectors for these new tokens, which can be trained.\n",
        " - `model = model.to(device)`: Moves the entire model (including the potentially resized embedding layer) from the CPU (where it was initially loaded) to the target computation device (`cuda` / GPU). This is necessary for GPU-accelerated training."
      ],
      "metadata": {
        "id": "LfqX9OTDf_Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.model_name,\n",
        "    torch_dtype=compute_dtype,\n",
        "    attn_implementation=\"eager\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"cpu\",\n",
        ")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZjjNiHjfw1o",
        "outputId": "b2a838b7-a147-40a9-f810-75793632e42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines a function `preprocess_and_filter` to prepare individual dataset samples for the `SFTTrainer`.\n",
        "\n",
        " - **Purpose:** To format the raw message data using the chat template and filter out sequences that are too long.\n",
        " - **Steps:**\n",
        "   1. Takes a `sample` (a dictionary expected to contain a \"messages\" key).\n",
        "   2. Extracts the `messages` list.\n",
        "   3. Uses `tokenizer.apply_chat_template(messages, tokenize=False)` to convert the list of message dictionaries into a single formatted string according to the template defined in step 7.\n",
        "   4. Encodes the resulting `text` into token IDs using `tokenizer.encode(text, truncation=False)`. Crucially, `truncation=False` is used here to get the *full* token length.\n",
        "   5. Checks if the number of tokens (`len(tokens)`) is less than or equal to the configured `max_seq_length` from `config.training_arguments`.\n",
        "   6. **If within limit:** Returns a dictionary `{\"text\": text}` containing the formatted string. `SFTTrainer` typically expects input data in a column named \"text\".\n",
        "   7. **If too long:** Returns `None`. This signals to the subsequent `.filter()` operation that this sample should be discarded.\n",
        " - **Rationale:** Ensures that all sequences used for training fit within the model's context window (`max_seq_length`), preventing errors and avoiding unwanted truncation by the trainer later. Filtering upfront is generally cleaner."
      ],
      "metadata": {
        "id": "SF-wbJ2fgjJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_filter(sample):\n",
        "  \"\"\"Preprocesses and filters a sample based on token length\"\"\"\n",
        "  messages = sample[\"messages\"]\n",
        "  text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "  tokens = tokenizer.encode(text, truncation=False)\n",
        "\n",
        "  if len(tokens) <= config.training_arguments[\"max_seq_length\"]:\n",
        "    return {\"text\": text}\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "Lsv2hUagd-CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = (\n",
        "        load_dataset(config.dataset_name)\n",
        "        .rename_column(\"conversations\", \"messages\")\n",
        "        .map(preprocess_and_filter, remove_columns=\"messages\")\n",
        "        .filter(lambda x: x is not None, keep_in_memory=False)\n",
        "    )"
      ],
      "metadata": {
        "id": "E31WumStd-CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell splits the processed dataset into training and testing subsets.\n",
        "\n",
        "- `data[\"train\"].train_test_split(0.2)`: Takes the 'train' split of the loaded and processed `data` (assuming the original dataset had a 'train' split) and splits it further. 80% of the data is kept for training (becomes the new 'train' split), and 20% is held out for evaluation (becomes the 'test' split).\n",
        "- **Purpose:** Creating separate train and test sets is crucial for evaluating the model's generalization performance. The model learns from the 'train' set, and its performance on the unseen 'test' set indicates how well it might perform on new, similar data. A 80/20 split is a common practice."
      ],
      "metadata": {
        "id": "DRwTkqFv4lU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = data[\"train\"].train_test_split(0.2)"
      ],
      "metadata": {
        "id": "jnoMXRGLd-CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell sets up the configuration for Parameter-Efficient Fine-Tuning (PEFT) using the LoRA technique.\n",
        "- `LoraConfig(...)`: Creates an instance of the LoRA configuration class.\n",
        "- `**config.lora_arguments`: Unpacks the dictionary of LoRA-specific hyperparameters (`r`, `lora_alpha`, `lora_dropout`, `target_modules`) defined earlier in the main `Config` class.\n",
        "- `task_type=TaskType.CAUSAL_LM`: Explicitly specifies that the PEFT technique (LoRA) is being applied to a Causal Language Model. This helps `peft` configure the model adaptation correctly for generation tasks.\n",
        "- **Purpose:** This `peft_config` object contains all the necessary information for the `SFTTrainer` to modify the base model by injecting LoRA adapters according to the specified parameters."
      ],
      "metadata": {
        "id": "wsU8VLXJ4qLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "        **config.lora_arguments,\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "    )"
      ],
      "metadata": {
        "id": "nok_J5xnd-CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell initializes the configuration object specifically required by the `SFTTrainer`.\n",
        "\n",
        " - `SFTConfig(...)`: Creates an instance of `SFTConfig`, which is a subclass of `transformers.TrainingArguments` tailored for the `SFTTrainer`.\n",
        " - `**config.training_arguments`: Unpacks the dictionary of general training hyperparameters (learning rate, batch size, epochs, optimization settings, logging, saving strategies, etc.) defined in the main `Config` class.\n",
        " - `output_dir=config.output_dir`: Explicitly sets the output directory where checkpoints and logs will be saved.\n",
        " - `fp16=config.fp16`, `bf16=config.bf16`: Sets the mixed-precision training flags based on the main configuration.\n",
        " - **Purpose:** This `training_arguments` object gathers all settings related to the training loop itself (optimization, scheduling, evaluation, saving, logging, etc.) into the format expected by the `SFTTrainer`.\n",
        "\n",
        " In addition:\n",
        "- `model.config.use_cache = False`: Disables the Key/Value (KV) cache mechanism in the model's attention layers. The KV cache speeds up *inference* by reusing past computations, but it's not needed during *training* and consumes significant GPU memory. Disabling it frees up memory, which is often crucial, especially when using gradient checkpointing.\n",
        "- `model.config.pretraining_tp = 1`: Sets the `pretraining_tp` (tensor parallelism used during pre-training) value to 1. This setting can sometimes be necessary for compatibility when fine-tuning models that were originally pre-trained with tensor parallelism, especially if the fine-tuning setup doesn't use the same degree of parallelism. Setting it to 1 essentially tells the configuration not to expect weights sharded in a particular way due to pre-training parallelism.\n",
        "- **Purpose:** These settings optimize the model configuration for the training phase, primarily focusing on memory efficiency (`use_cache=False`) and potential compatibility (`pretraining_tp=1`)."
      ],
      "metadata": {
        "id": "L2EhP4H35aM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = SFTConfig(\n",
        "    **config.training_arguments,\n",
        "    output_dir=config.output_dir,\n",
        "    fp16=config.fp16,\n",
        "    bf16=config.bf16,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "9VM-RRw55CK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell creates the `SFTTrainer` instance, which will manage the fine-tuning process.\n",
        "\n",
        "- `SFTTrainer(...)`: Initializes the trainer class from the `trl` library.\n",
        "- **Arguments:**\n",
        "   - `model=model`: The language model to be fine-tuned. The `peft` library will automatically modify this model based on `peft_config` when training starts.\n",
        "   - `args=training_arguments`: The `SFTConfig` object containing all training hyperparameters and settings.\n",
        "   - `train_dataset=dataset[\"train\"]`: The dataset split to be used for training.\n",
        "   - `eval_dataset=dataset[\"test\"]`: The dataset split to be used for evaluation.\n",
        "   - `tokenizer=tokenizer`: The tokenizer to be used for processing data (though much preprocessing was done manually here, the trainer might use it for collation or other internal steps). `processing_class` seems like a typo and likely should be `tokenizer`. Assuming it means `tokenizer`.\n",
        "   - `peft_config=peft_config`: The `LoraConfig` object specifying how LoRA should be applied. Passing this instructs the trainer to use PEFT.\n",
        "- **Purpose:** The `SFTTrainer` object encapsulates the model, data, tokenizer, and all configurations needed to run the supervised fine-tuning loop, handle evaluation, checkpointing, and logging.\n",
        "\n",
        "Then the cell initiates the actual fine-tuning process.\n",
        "\n",
        " - `trainer.train()`: Calls the `train` method of the `SFTTrainer` instance.\n",
        " - **Action:** This starts the training loop. The trainer will:\n",
        "   - Apply the LoRA modifications to the model based on `peft_config`.\n",
        "   - Iterate through the `train_dataset` for the specified number of epochs/steps.\n",
        "   - Compute loss, perform backpropagation, and update the LoRA adapter weights (and any other trainable parameters like embeddings).\n",
        "   - Perform evaluation on the `eval_dataset` based on the `eval_strategy`.\n",
        "   - Save model checkpoints based on the `save_strategy`.\n",
        "   - Log metrics according to `logging_steps` and `report_to`.\n",
        "   - Finally, load the best checkpoint if `load_best_model_at_end=True`.\n"
      ],
      "metadata": {
        "id": "fCGlHD-Q5qpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=peft_config,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "83de8dcb-8c73-40e8-dbe7-103390c91055",
        "id": "RS4QydkKd-CT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:550: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.embed_tokens', 'lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
            "  warnings.warn(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='279' max='279' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [279/279 2:15:02, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.312400</td>\n",
              "      <td>0.310175</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=279, training_loss=0.5116325629654751, metrics={'train_runtime': 8128.7487, 'train_samples_per_second': 0.138, 'train_steps_per_second': 0.034, 'total_flos': 9849852881074752.0, 'train_loss': 0.5116325629654751})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell saves the results of the fine-tuning process locally.\n",
        "\n",
        " - `trainer.model.save_pretrained(\"LoRA_\" + config.output_dir, save_embedding_layers=True)`: Saves the trained PEFT adapter weights (the LoRA layers) to a directory named \"LoRA_gemma-3-1B-it-function_calling\". Because LoRA was used, this saves only the small adapter weights, not the entire base model. `save_embedding_layers=True` attempts to save the fine-tuned input/output embedding layers if they were targeted by LoRA or resized and made trainable; this behavior can vary across library versions.\n",
        " - `tokenizer.eos_token = \"<eos>\"`: Explicitly sets the `eos_token` attribute of the tokenizer object. This might be redundant if already configured but acts as a safeguard.\n",
        " - `tokenizer.save_pretrained(\"LoRA_\" + config.output_dir)`: Saves the tokenizer's configuration (including vocabulary, added special tokens like the ChatML ones, and the custom chat template) to the same directory as the LoRA adapters.\n",
        " - **Purpose:** Persists the fine-tuning results (adapter weights) and the corresponding tokenizer configuration needed to correctly load and use the fine-tuned model later for inference. Saving them together ensures compatibility."
      ],
      "metadata": {
        "id": "nioMHvp96Cel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving LoRA weights and tokenizer\n",
        "trainer.model.save_pretrained(\n",
        "    \"LoRA_\" + config.output_dir, save_embedding_layers=True\n",
        ")\n",
        "tokenizer.eos_token = \"<eos>\"\n",
        "tokenizer.save_pretrained(\"LoRA_\" + config.output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b64164-c1fc-4237-f4a6-db32af0a03a0",
        "id": "0WEsyn5ld-CT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('LoRA_gemma-3-1B-it-function_calling/tokenizer_config.json',\n",
              " 'LoRA_gemma-3-1B-it-function_calling/special_tokens_map.json',\n",
              " 'LoRA_gemma-3-1B-it-function_calling/tokenizer.model',\n",
              " 'LoRA_gemma-3-1B-it-function_calling/added_tokens.json',\n",
              " 'LoRA_gemma-3-1B-it-function_calling/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell handles authentication with the Hugging Face Hub, using secrets management within Google Colab.\n",
        "\n",
        " - `from huggingface_hub import login`: Imports the login function.\n",
        " - `from google.colab import userdata`: Imports the utility for accessing secrets stored in Colab.\n",
        " - `userdata.get('HF_TOKEN')`: Attempts to retrieve a secret named 'HF_TOKEN', which should contain a Hugging Face API token with write permissions.\n",
        " - `login(hf_token)`: If the token is found, this function authenticates the Colab environment with the Hugging Face Hub, allowing subsequent push operations.\n",
        " - **Purpose:** Securely authenticates the session to allow uploading the fine-tuned adapter and tokenizer to a user's repository on the Hugging Face Hub. Using secrets avoids hardcoding sensitive tokens in the notebook."
      ],
      "metadata": {
        "id": "krHlf8oR6DFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if hf_token:\n",
        "    login(hf_token)\n",
        "    print(\"Successfully logged in!\")\n",
        "else:\n",
        "    print(\"Token not found. Check Secrets configuration.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe731ea-af31-4927-acb4-61b1ae0036cf",
        "id": "4-JzB3gid-CT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully logged in!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell uploads the saved LoRA adapter weights and the tokenizer configuration to the specified repository on the Hugging Face Hub.\n",
        "\n",
        " - `username=\"xxxxxx\"`: **Placeholder:** Replace `\"xxxxxx\"` with the actual Hugging Face Hub username.\n",
        " - `output_dir = \"gemma-3-1B-it-function_calling\"`: The desired name for the repository on the Hub.\n",
        " - `trainer.push_to_hub(f\"{username}/{output_dir}\")`: Uploads the contents of the local directory where the adapter was saved (by `trainer.model.save_pretrained`) to the specified Hub repository (`username/output_dir`). This includes the adapter weights (`adapter_model.safetensors`) and configuration (`adapter_config.json`).\n",
        " - `tokenizer.push_to_hub(f\"{username}/{output_dir}\", token=True)`: Uploads the tokenizer files (saved by `tokenizer.save_pretrained`) to the *same* Hub repository. `token=True` ensures the authentication token is used, though it might be implicit after `login`.\n",
        " - **Purpose:** Makes the fine-tuned LoRA adapter and its corresponding tokenizer publicly or privately accessible via the Hugging Face Hub, facilitating sharing, collaboration, and easy loading for inference elsewhere."
      ],
      "metadata": {
        "id": "B32GItfJ6TeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "username=\"lmassaron\"\n",
        "output_dir = \"gemma-3-1B-it-function_calling\"\n",
        "trainer.push_to_hub(f\"{username}/{output_dir}\")\n",
        "tokenizer.push_to_hub(f\"{username}/{output_dir}\", token=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308,
          "referenced_widgets": [
            "8a6f2dbe732d47d5ac3bd4e063e43cfa",
            "9c5461fcb6e24b049c11f4555a7b02a3",
            "49e271c17c05460db9a18ca14ab09672",
            "a922078ae5184df0a14076e04ff2954e",
            "fa2e51a8ced44e228077ef7ff0e1f3ce",
            "cd37c80b741a4f2f82955d861224be3b",
            "319e82cbb49f4ccb856cc530691b34ce",
            "b2ec36dd3e29466ca531758b0966bd73",
            "8fe3a14debf04839a327ab57646e582e",
            "0c1ae2a8206049fca3bb758b9efc3ab2",
            "4553d3b338c04cd68fc84a36ef2876d3",
            "abba2dedec844721bdbf18264d1236e3",
            "e8e0cd220a00480491b17ed9e3197a45",
            "09b84bf712d84f758c73e2d05e191ef9",
            "f1a158dccfc14811a23bde2923c2b759",
            "91d53d3308a64c16ad7696a34ff1fdd2",
            "e98c7e76947c4813b1ad29c67cee50a3",
            "f5110673119a4999b08d9d40e6dff801",
            "a3cc88ab86ae4bb08526688487b6665e",
            "65c7b792b5904c328efe2138b0575a23",
            "5c3650281ad345d198988db43c46487c",
            "4102c0a204b14572974c99b094931bd9",
            "6760ad37840f4e368c1d35b02e255930",
            "32b09548287f4839aaf5dd33a06c43c9",
            "efa0f0c420a148bbb7554b42c188a62f",
            "7fe6c9c063c44830b8125b0e1f7aebbf",
            "b675337d45234c3da23cc1910ff45d5e",
            "ce8fa61404e142fbb59c5e314cf987dc",
            "3699082f904d4319a293745d17f1c5cb",
            "2e39455584b74ec59914fc9e08397875",
            "db59bd0358f84740bb90cce9feda0ba8",
            "97a58047708a47b190584d1767049902",
            "185f30d2dd3e4276942f356663a6db62",
            "d0f0466c2705494bae6fe10bcc70e9cb",
            "42d8aa5b21884e0ab1a43b47aaa65c5c",
            "e73c78f0ae1d471b8e69dc52f27ca3e4",
            "2b8b22fc5f1743f1a96ba55989460d88",
            "bef2be5b4a6b474f8c37dcd8afd82873",
            "067fef76009d42c6ad5f08a0c94af3dd",
            "9313cf536a2a481090c326b88b0c6c88",
            "a5c96f69027d41d2843e930d829f856a",
            "f977c73bedf94dbe914152512414c193",
            "4b183c01d1ec4f218b31a147023cb71a",
            "d9caf98009764f7ab9215f4228359933"
          ]
        },
        "outputId": "10a88f6f-1f0d-4f64-9c91-7802624df4eb",
        "id": "uEbQIkEXd-CT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:211: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a6f2dbe732d47d5ac3bd4e063e43cfa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/1.29G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abba2dedec844721bdbf18264d1236e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "training_args.bin:   0%|          | 0.00/5.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6760ad37840f4e368c1d35b02e255930"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/1.50k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0f0466c2705494bae6fe10bcc70e9cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/lmassaron/gemma-3-1B-it-function_calling/commit/33813b064fe04f8675a96f3dc984069930e95752', commit_message='Upload tokenizer', commit_description='', oid='33813b064fe04f8675a96f3dc984069930e95752', pr_url=None, repo_url=RepoUrl('https://huggingface.co/lmassaron/gemma-3-1B-it-function_calling', endpoint='https://huggingface.co', repo_type='model', repo_id='lmassaron/gemma-3-1B-it-function_calling'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}